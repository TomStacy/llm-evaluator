{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73993a9",
   "metadata": {},
   "source": [
    "# LLM Model Comparison using OpenRouter Rankings\n",
    "\n",
    "This notebook evaluates and compares the top language models from [OpenRouter Rankings](https://openrouter.ai/rankings).\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Fetches Top Models** - Scrapes the current rankings from OpenRouter to identify the most popular models\n",
    "2. **Generates Questions** - Each model creates a challenging reasoning question\n",
    "3. **Answers Questions** - Each model answers all questions from other models\n",
    "4. **Evaluates Responses** - Each model rates the quality of all answers on a 10-point scale\n",
    "5. **Aggregates Results** - Produces comparison metrics and cross-model rating matrices\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **OpenRouter API Key**: Set the `OPENROUTER_API_KEY` environment variable\n",
    "- **Dependencies**: pandas, requests, beautifulsoup4, playwright, openai (automatically installed with `uv sync`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for the notebook\n",
    "import os  # Environment variable access\n",
    "from typing import Any  # Type hints\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd  # DataFrames for structured data\n",
    "\n",
    "# Web scraping and API calls\n",
    "import requests  # HTTP requests for OpenRouter API\n",
    "from bs4 import BeautifulSoup  # HTML parsing for rankings page\n",
    "\n",
    "# Note: Additional imports (re, time, playwright, openai) are imported\n",
    "# within specific cells where they're needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb12d0e",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline Flow\n",
    "\n",
    "Here's how the evaluation works step-by-step:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 1: Fetch Top Models from OpenRouter Rankings      │\n",
    "│  → Scrapes live usage data OR uses API sorting          │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 2: Question Generation (N models)                 │\n",
    "│  → Each model creates 1 challenging question            │\n",
    "│  → Total: N questions                                   │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 3: Answer Generation (N × N combinations)         │\n",
    "│  → Each model answers every question                    │\n",
    "│  → Total: N × N answers                                 │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 4: Answer Evaluation (N × N x N ratings)          │\n",
    "│  → Each model rates every answer (1-10 scale)           │\n",
    "│  → Total: N × N x N ratings                             │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 5: Aggregate Results                              │\n",
    "│  → Average ratings per model                            │\n",
    "│  → Cross-model rating matrix                            │\n",
    "│  → Performance insights                                 │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Example with 5 models:**\n",
    "- 5 questions generated\n",
    "- 25 answers generated (5 models × 5 questions)\n",
    "- 125 ratings collected (5 models × 5 answers x 5 questions)\n",
    "- Total LLM calls: 155"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b9032",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. **Install Playwright browsers** (one-time setup):\n",
    "```bash\n",
    "uv run playwright install chromium\n",
    "```\n",
    "\n",
    "2. **Set your OpenRouter API Key**:\n",
    "   - Get a free API key from [OpenRouter](https://openrouter.ai/)\n",
    "   - Set it as an environment variable:\n",
    "     - **Windows (PowerShell)**: `$env:OPENROUTER_API_KEY=\"your-key-here\"`\n",
    "     - **Mac/Linux**: `export OPENROUTER_API_KEY=\"your-key-here\"`\n",
    "   - Or add it to a `.env` file in the project root\n",
    "\n",
    "The notebook will check for this API key before making requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9705252",
   "metadata": {},
   "source": [
    "## Fetch Top Models from OpenRouter\n",
    "\n",
    "This section scrapes the OpenRouter rankings page to get real-time popularity data based on actual usage.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The `fetch_openrouter_rankings()` function uses **Playwright** (a browser automation tool) to:\n",
    "1. Launch a headless browser\n",
    "2. Navigate to the OpenRouter rankings page\n",
    "3. Wait for JavaScript content to load\n",
    "4. Extract model data including rank, name, token usage, and usage trends\n",
    "\n",
    "**Why Playwright?** The rankings page uses JavaScript to render content dynamically, so we need a real browser to see the data.\n",
    "\n",
    "**Windows Event Loop Note:** Jupyter uses an asyncio event loop that conflicts with Playwright on Windows. The function runs Playwright in a separate thread with its own event loop to avoid this issue.\n",
    "\n",
    "### Available Sorting Options\n",
    "\n",
    "The `fetch_top_openrouter_models()` function supports different sorting criteria:\n",
    "- **popularity** - Models ranked by actual usage on OpenRouter (default, uses Playwright scraping)\n",
    "- **price_low** - Cheapest models first\n",
    "- **price_high** - Most expensive (often most capable) models first  \n",
    "- **context_length** - Longest context window first\n",
    "- **newest** - Most recently added models first\n",
    "\n",
    "**Note:** Only the popularity ranking requires Playwright. Other sorting options use the OpenRouter API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff56820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback model list used when API calls or web scraping fails\n",
    "FALLBACK_MODELS = [\n",
    "    {\n",
    "        \"id\": \"anthropic/claude-3.5-sonnet\",\n",
    "        \"name\": \"Claude 3.5 Sonnet\",\n",
    "        \"description\": \"Anthropic Claude 3.5 Sonnet\",\n",
    "        \"context_length\": 200000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"openai/gpt-4o\",\n",
    "        \"name\": \"GPT-4o\",\n",
    "        \"description\": \"OpenAI GPT-4o\",\n",
    "        \"context_length\": 128000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"google/gemini-pro-1.5\",\n",
    "        \"name\": \"Gemini Pro 1.5\",\n",
    "        \"description\": \"Google Gemini Pro 1.5\",\n",
    "        \"context_length\": 1_000_000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"meta-llama/llama-3.1-405b-instruct\",\n",
    "        \"name\": \"Llama 3.1 405B\",\n",
    "        \"description\": \"Meta Llama 3.1 405B Instruct\",\n",
    "        \"context_length\": 128000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"anthropic/claude-3.5-haiku\",\n",
    "        \"name\": \"Claude 3.5 Haiku\",\n",
    "        \"description\": \"Anthropic Claude 3.5 Haiku\",\n",
    "        \"context_length\": 100000,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openrouter_rankings() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape the OpenRouter rankings page to get the current top models by actual usage.\n",
    "    Uses Playwright async API run in a separate thread to avoid event loop conflicts.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: rank, model_id, model_name, tokens, token_change\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    import concurrent.futures\n",
    "    import sys\n",
    "\n",
    "    async def _async_fetch():\n",
    "        \"\"\"Internal async function to fetch rankings using browser automation\"\"\"\n",
    "        try:\n",
    "            import re\n",
    "\n",
    "            from playwright.async_api import async_playwright\n",
    "\n",
    "            print(\"Launching browser to fetch rankings...\")\n",
    "\n",
    "            async with async_playwright() as p:\n",
    "                # Launch browser in headless mode (no visible window)\n",
    "                browser = await p.chromium.launch(headless=True)\n",
    "                page = await browser.new_page()\n",
    "\n",
    "                try:\n",
    "                    # Navigate to rankings page with short timeout (user can retry if it fails)\n",
    "                    await page.goto(\n",
    "                        \"https://openrouter.ai/rankings\",\n",
    "                        wait_until=\"domcontentloaded\",\n",
    "                        timeout=15000,\n",
    "                    )\n",
    "\n",
    "                    # Wait for the leaderboard section to appear in the DOM\n",
    "                    await page.wait_for_selector(\"#leaderboard\", timeout=10000, state=\"attached\")\n",
    "\n",
    "                    # Give JavaScript time to populate the content (5 seconds)\n",
    "                    await page.wait_for_timeout(5000)\n",
    "\n",
    "                    # Get the fully rendered HTML after JavaScript execution\n",
    "                    html = await page.content()\n",
    "\n",
    "                finally:\n",
    "                    await browser.close()\n",
    "\n",
    "            # Parse the rendered HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            leaderboard = soup.find(id=\"leaderboard\")\n",
    "\n",
    "            if not leaderboard:\n",
    "                print(\"⚠ Leaderboard section not found in rendered page\")\n",
    "                return pd.DataFrame(columns=[\"rank\", \"model_id\", \"model_name\", \"tokens\", \"token_change\"])\n",
    "\n",
    "            # Extract model information using CSS selectors\n",
    "            rankings_data: list[dict[str, object]] = []\n",
    "\n",
    "            # Find all leaderboard entries (each entry is a grid container with 12 columns)\n",
    "            entries = leaderboard.select(\"div.grid.grid-cols-12.items-center\")\n",
    "\n",
    "            for entry in entries[:30]:  # Limit to top 30 models\n",
    "                try:\n",
    "                    # Column 1: Extract rank number (e.g., \"1.\", \"2.\")\n",
    "                    rank_elem = entry.select_one(\"div.col-span-1\")\n",
    "                    rank = int(rank_elem.get_text(strip=True).replace(\".\", \"\")) if rank_elem else None\n",
    "\n",
    "                    # Column 2: Extract model name and ID from the link\n",
    "                    model_link = entry.select_one(\"div.col-span-7 a.font-medium\")\n",
    "                    if not model_link:\n",
    "                        continue\n",
    "\n",
    "                    model_name = model_link.get_text(strip=True)\n",
    "                    href = model_link.get(\"href\", \"\")\n",
    "                    # Remove leading \"/\" from href to get model_id\n",
    "                    model_id = href[1:] if isinstance(href, str) and href.startswith(\"/\") else href\n",
    "\n",
    "                    # Column 3: Extract token count and change percentage\n",
    "                    tokens: int | None = None\n",
    "                    token_change: str | None = None\n",
    "                    token_container = entry.select_one(\"div.col-span-4\")\n",
    "\n",
    "                    if token_container:\n",
    "                        divs = token_container.select(\"div\")\n",
    "\n",
    "                        # First div contains the token count (e.g., \"1.04T tokens\", \"801B tokens\")\n",
    "                        if divs:\n",
    "                            token_text = divs[0].get_text(strip=True)\n",
    "                            # Parse token count with units (K=thousand, M=million, B=billion, T=trillion)\n",
    "                            token_match = re.search(r\"([\\d.]+)([KMBT])\\s*tokens\", token_text, re.IGNORECASE)\n",
    "                            if token_match:\n",
    "                                value = float(token_match.group(1))\n",
    "                                unit = token_match.group(2).upper()\n",
    "                                multipliers = {\n",
    "                                    \"K\": 1_000,\n",
    "                                    \"M\": 1_000_000,\n",
    "                                    \"B\": 1_000_000_000,\n",
    "                                    \"T\": 1_000_000_000_000,\n",
    "                                }\n",
    "                                tokens = int(value * multipliers.get(unit, 1))\n",
    "\n",
    "                        # Extract percentage change (usage trend)\n",
    "                        percent_div = token_container.select_one(\"div.mt-1\")\n",
    "                        if percent_div:\n",
    "                            svg_elem = percent_div.select_one(\"svg\")\n",
    "                            full_text = percent_div.get_text(strip=True)\n",
    "                            percent_match = re.search(r\"([\\d.]+)%\", full_text)\n",
    "                            if percent_match:\n",
    "                                percentage_value = percent_match.group(1)\n",
    "                                svg_class = \"\"\n",
    "                                if svg_elem:\n",
    "                                    svg_class_raw = svg_elem.get(\"class\", [])\n",
    "                                    if isinstance(svg_class_raw, list):\n",
    "                                        svg_class = \" \".join(svg_class_raw)\n",
    "                                    else:  # str\n",
    "                                        svg_class = str(svg_class_raw)\n",
    "                                # Red SVG indicates decrease, green indicates increase\n",
    "                                if svg_class and \"text-red\" in svg_class:\n",
    "                                    token_change = f\"-{percentage_value}%\"\n",
    "                                else:\n",
    "                                    token_change = f\"{percentage_value}%\"\n",
    "\n",
    "                    rankings_data.append(\n",
    "                        {\n",
    "                            \"rank\": rank,\n",
    "                            \"model_id\": model_id,\n",
    "                            \"model_name\": model_name,\n",
    "                            \"tokens\": tokens,\n",
    "                            \"token_change\": token_change,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                except (ValueError, AttributeError):\n",
    "                    # Skip entries that don't match expected format\n",
    "                    continue\n",
    "\n",
    "            if rankings_data:\n",
    "                print(f\"✓ Successfully extracted {len(rankings_data)} models from rankings page\")\n",
    "                return pd.DataFrame(rankings_data)\n",
    "\n",
    "            print(\"⚠ No models found, using fallback\")\n",
    "            raise ValueError(\"No models extracted\")\n",
    "\n",
    "        except Exception as err:  # noqa: BLE001\n",
    "            print(f\"Error with Playwright: {err}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _run_in_thread():\n",
    "        \"\"\"\n",
    "        Run async function in a new event loop in a separate thread.\n",
    "        This avoids conflicts with Jupyter's event loop on Windows.\n",
    "        \"\"\"\n",
    "        # On Windows, use ProactorEventLoop which supports subprocesses\n",
    "        if sys.platform == \"win32\":\n",
    "            loop = asyncio.WindowsProactorEventLoopPolicy().new_event_loop()\n",
    "        else:\n",
    "            loop = asyncio.new_event_loop()\n",
    "\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            return loop.run_until_complete(_async_fetch())\n",
    "        finally:\n",
    "            loop.close()\n",
    "\n",
    "    try:\n",
    "        # Run in a thread pool to avoid event loop conflicts with Jupyter\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(_run_in_thread)\n",
    "            return future.result(timeout=30)\n",
    "\n",
    "    except Exception as err:  # noqa: BLE001\n",
    "        print(f\"Error fetching rankings: {err}\")\n",
    "\n",
    "        # Fallback to hardcoded list if scraping fails\n",
    "        print(\"Using fallback: hardcoded top models list\")\n",
    "        fallback_rankings = [\n",
    "            {\n",
    "                \"rank\": i + 1,\n",
    "                \"model_id\": model[\"id\"],\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"tokens\": None,\n",
    "                \"token_change\": None,\n",
    "            }\n",
    "            for i, model in enumerate(FALLBACK_MODELS)\n",
    "        ]\n",
    "        return pd.DataFrame(fallback_rankings)\n",
    "\n",
    "\n",
    "def fetch_top_openrouter_models(top_n: int = 5, sort_by: str = \"popularity\") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch the top N models from OpenRouter using various sorting criteria.\n",
    "\n",
    "    Args:\n",
    "        top_n: Number of top models to return\n",
    "        sort_by: Sorting criterion - 'popularity', 'price_low', 'price_high',\n",
    "                'context_length', 'newest'\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing model information with keys:\n",
    "        id, name, description, context_length, pricing, created, avg_cost\n",
    "    \"\"\"\n",
    "    url = \"https://openrouter.ai/api/v1/models\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract model data from API response\n",
    "        models = data.get(\"data\", [])\n",
    "\n",
    "        # Filter out models without proper pricing or context info\n",
    "        valid_models: list[dict[str, object]] = []\n",
    "        for model in models:\n",
    "            pricing = model.get(\"pricing\", {})\n",
    "            # Only include models with valid pricing information\n",
    "            if pricing and pricing.get(\"prompt\") and pricing.get(\"completion\"):\n",
    "                try:\n",
    "                    prompt_cost = float(pricing.get(\"prompt\", \"0\"))\n",
    "                    completion_cost = float(pricing.get(\"completion\", \"0\"))\n",
    "                except (TypeError, ValueError):\n",
    "                    continue\n",
    "                model_info = {\n",
    "                    \"id\": model.get(\"id\", \"\"),\n",
    "                    \"name\": model.get(\"name\", model.get(\"model_id\", \"\")),\n",
    "                    \"description\": model.get(\"description\", \"No description available\"),\n",
    "                    \"context_length\": model.get(\"context_length\", 0),\n",
    "                    \"pricing\": pricing,\n",
    "                    \"created\": model.get(\"created\", 0),\n",
    "                    # Calculate average cost per 1M tokens for easy comparison\n",
    "                    \"avg_cost\": (prompt_cost + completion_cost) / 2 * 1_000_000,\n",
    "                }\n",
    "                valid_models.append(model_info)\n",
    "\n",
    "        # Sort models based on the specified criterion\n",
    "        if sort_by == \"popularity\":\n",
    "            print(\"Fetching current model rankings from OpenRouter (using Playwright)...\")\n",
    "            rankings_df = fetch_openrouter_rankings()\n",
    "            if rankings_df is not None and not rankings_df.empty:\n",
    "                # Create a mapping of model_id to rank\n",
    "                popularity_order = {row[\"model_id\"]: row[\"rank\"] for _, row in rankings_df.iterrows()}\n",
    "                # Sort by rank (lower rank = more popular), default to 999 for unranked\n",
    "                sorted_models = sorted(valid_models, key=lambda x: popularity_order.get(x[\"id\"], 999))\n",
    "                print(f\"✓ Successfully ranked {len(rankings_df)} models by current usage data\")\n",
    "            else:\n",
    "                print(\"⚠ Could not fetch rankings, using default sort\")\n",
    "                sorted_models = valid_models\n",
    "        elif sort_by == \"price_low\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"avg_cost\"])\n",
    "        elif sort_by == \"price_high\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"avg_cost\"], reverse=True)\n",
    "        elif sort_by == \"context_length\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"context_length\"], reverse=True)\n",
    "        elif sort_by == \"newest\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"created\"], reverse=True)\n",
    "        else:\n",
    "            sorted_models = valid_models\n",
    "\n",
    "        return sorted_models[:top_n]\n",
    "\n",
    "    except Exception as err:  # noqa: BLE001\n",
    "        print(f\"Error fetching models: {err}\")\n",
    "        # Fallback to a hardcoded list of popular models\n",
    "        return [{**model, \"pricing\": {}, \"avg_cost\": 0} for model in FALLBACK_MODELS][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbeae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Fetch and display the current rankings\n",
    "rankings_df = fetch_openrouter_rankings()\n",
    "\n",
    "print(f\"Successfully fetched {len(rankings_df)} ranked models\\n\")\n",
    "print(\"Top 10 Models by Usage on OpenRouter:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display_df = rankings_df.head(10).copy()\n",
    "\n",
    "\n",
    "def format_tokens(tokens: int | None) -> str:\n",
    "    if tokens is None:\n",
    "        return \"N/A\"\n",
    "    if tokens >= 1_000_000_000_000:\n",
    "        return f\"{tokens / 1_000_000_000_000:.2f}T\"\n",
    "    if tokens >= 1_000_000_000:\n",
    "        return f\"{tokens / 1_000_000_000:.1f}B\"\n",
    "    if tokens >= 1_000_000:\n",
    "        return f\"{tokens / 1_000_000:.1f}M\"\n",
    "    return f\"{tokens:,}\"\n",
    "\n",
    "\n",
    "display_df[\"tokens_formatted\"] = display_df[\"tokens\"].apply(format_tokens)\n",
    "display_df[[\"rank\", \"model_name\", \"model_id\", \"tokens_formatted\", \"token_change\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92763d89",
   "metadata": {},
   "source": [
    "### Understanding the Rankings Output\n",
    "\n",
    "The table above shows:\n",
    "\n",
    "- **rank**: Current position on OpenRouter (lower = more popular)\n",
    "- **model_name**: Human-readable name of the model\n",
    "- **model_id**: Unique identifier used for API calls (format: `provider/model-name`)\n",
    "- **tokens_formatted**: Total tokens processed (T=trillion, B=billion, M=million)\n",
    "- **token_change**: Usage trend as percentage change (↑ green positive, ↓ red negative)\n",
    "\n",
    "**What does this tell us?**\n",
    "Models at the top are being used most heavily by real users on OpenRouter, which often (but not always) correlates with quality, speed, or value. This ranking updates in real-time based on actual API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \":free\" suffix from model IDs for cleaner display\n",
    "# OpenRouter sometimes appends \":free\" to free-tier models\n",
    "display_df[\"model_id\"] = display_df[\"model_id\"].str.replace(r\":free$\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23decdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 5 models from the rankings for our evaluation\n",
    "# Alternative approach (commented out): fetch via API with different sorting\n",
    "# top_models = fetch_top_openrouter_models(5, sort_by=\"popularity\")\n",
    "# df_models = pd.DataFrame(top_models)\n",
    "\n",
    "top_models = display_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c313c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that we have the required data for the evaluation\n",
    "print(\"Data Validation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check we have models\n",
    "if len(display_df) < 3:\n",
    "    print(\"⚠️  Warning: Less than 3 models available\")\n",
    "    print(\"   Results may be less meaningful with fewer models\")\n",
    "else:\n",
    "    print(f\"✓ {len(display_df)} models available for evaluation\")\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = [\"model_id\", \"model_name\"]\n",
    "missing_cols = [col for col in required_cols if col not in display_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Check for duplicate models\n",
    "duplicates = display_df[\"model_id\"].duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"⚠️  Warning: {duplicates} duplicate model IDs found\")\n",
    "else:\n",
    "    print(\"✓ No duplicate models\")\n",
    "\n",
    "print(\"\\nReady to proceed with evaluation! 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the OpenRouter API client\n",
    "# OpenRouter uses an OpenAI-compatible API, so we use the OpenAI Python client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Get API key from environment variable\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    error_msg = \"\"\"\n",
    "    ❌ OPENROUTER_API_KEY not found!\n",
    "    \n",
    "    To fix this issue:\n",
    "    \n",
    "    1. Get a free API key from: https://openrouter.ai/\n",
    "       (Sign up with GitHub or email)\n",
    "    \n",
    "    2. Set the environment variable:\n",
    "       \n",
    "       Windows PowerShell:\n",
    "         $env:OPENROUTER_API_KEY=\"sk-or-v1-xxxxx\"\n",
    "       \n",
    "       Windows CMD:\n",
    "         set OPENROUTER_API_KEY=sk-or-v1-xxxxx\n",
    "       \n",
    "       Mac/Linux:\n",
    "         export OPENROUTER_API_KEY=\"sk-or-v1-xxxxx\"\n",
    "    \n",
    "    3. Restart this notebook kernel (Kernel → Restart)\n",
    "    \n",
    "    Alternative: Create a .env file in the project root:\n",
    "       OPENROUTER_API_KEY=sk-or-v1-xxxxx\n",
    "    \"\"\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Validate API key format\n",
    "if not OPENROUTER_API_KEY.startswith(\"sk-or-\"):\n",
    "    print(\"⚠️  Warning: API key doesn't start with 'sk-or-'\")\n",
    "    print(\"   This might not be a valid OpenRouter API key\")\n",
    "    print(\"   Expected format: sk-or-v1-xxxxx\")\n",
    "\n",
    "# Create client with OpenRouter's base URL\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OPENROUTER_API_KEY)\n",
    "\n",
    "print(\"✓ OpenRouter client configured successfully!\")\n",
    "print(f\"  API key: {OPENROUTER_API_KEY[:15]}...{OPENROUTER_API_KEY[-4:]}\")\n",
    "print(\"  Base URL: https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0d2fa",
   "metadata": {},
   "source": [
    "## Model Comparison Pipeline\n",
    "\n",
    "The evaluation consists of three phases:\n",
    "\n",
    "1. **Question Generation** - Each model creates one challenging reasoning question\n",
    "2. **Answer Generation** - Each model answers every question from all other models  \n",
    "3. **Answer Evaluation** - Each model rates every answer on a 10-point scale\n",
    "\n",
    "This creates a comprehensive cross-comparison where models evaluate each other's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Display how many models we're using for the evaluation pipeline\n",
    "num_models = len(top_models)\n",
    "total_questions = num_models\n",
    "total_answers = num_models * num_models\n",
    "total_ratings = num_models * num_models * total_questions\n",
    "total_api_calls = total_questions + total_answers + total_ratings\n",
    "\n",
    "print(f\"Using {num_models} models for the evaluation pipeline:\")\n",
    "print(\"  - Each model will generate 1 question\")\n",
    "print(f\"  - Each model will answer {num_models} questions\")\n",
    "print(f\"  - Each model will rate {total_answers} answers\")\n",
    "print(f\"\\nTotal API calls: {total_api_calls}\")\n",
    "print(f\"  • {total_questions} question generation calls\")\n",
    "print(f\"  • {total_answers} answer generation calls\")\n",
    "print(f\"  • {total_ratings} rating calls\")\n",
    "\n",
    "# Estimate time based on typical API response times\n",
    "avg_question_time = 3  # seconds\n",
    "avg_answer_time = 5  # seconds\n",
    "avg_rating_time = 2  # seconds\n",
    "\n",
    "estimated_time = total_questions * avg_question_time + total_answers * avg_answer_time + total_ratings * avg_rating_time\n",
    "\n",
    "print(f\"\\n⏱️  Estimated total time: ~{estimated_time // 60} minutes {estimated_time % 60} seconds\")\n",
    "print(f\"   (assuming {avg_question_time}s/question, {avg_answer_time}s/answer, {avg_rating_time}s/rating)\")\n",
    "print(\"\\n💡 Tip: Actual time may vary based on model speed and API load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cost_summary_markdown(summary_df:pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a markdown-formatted cost summary from the summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "        summary_df: DataFrame with cost and token usage data\n",
    "\n",
    "    Returns:\n",
    "        String containing markdown-formatted cost summary\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        return \"⚠️ No token data found in any DataFrames\\n\"\n",
    "\n",
    "    markdown_lines = []\n",
    "\n",
    "    # Header\n",
    "    markdown_lines.append(\"## 📊 TOKEN USAGE & COST SUMMARY (ACTUAL COSTS)\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    # Overall totals\n",
    "    total_input_tokens = summary_df[\"input_tokens\"].sum()\n",
    "    total_output_tokens = summary_df[\"output_tokens\"].sum()\n",
    "    total_tokens = summary_df[\"total_tokens\"].sum()\n",
    "    total_time = round(summary_df[\"time_s\"].sum(), 2)\n",
    "    total_cost = summary_df[\"actual_cost_usd\"].sum()\n",
    "    successful_fetches = summary_df[\"cost_fetch_success\"].sum()\n",
    "    total_calls = len(summary_df)\n",
    "\n",
    "    markdown_lines.append(\"## 🔢 OVERALL TOTALS\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(f\"- **Input tokens:** {total_input_tokens:,}\")\n",
    "    markdown_lines.append(f\"- **Output tokens:** {total_output_tokens:,}\")\n",
    "    markdown_lines.append(f\"- **Total tokens:** {total_tokens:,}\")\n",
    "    markdown_lines.append(f\"- **Total time (s):** {total_time:,}\")\n",
    "    markdown_lines.append(f\"- **Actual cost:** ${total_cost:.2f}\")\n",
    "    markdown_lines.append(\n",
    "        f\"- **Cost data success:** {successful_fetches}/{total_calls} calls ({successful_fetches/total_calls*100:.1f}%)\"\n",
    "    )\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    # By phase breakdown\n",
    "    phase_summary = (\n",
    "        summary_df.groupby(\"phase\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"input_tokens\": \"sum\",\n",
    "                \"output_tokens\": \"sum\",\n",
    "                \"total_tokens\": \"sum\",\n",
    "                \"actual_cost_usd\": \"sum\",\n",
    "                \"cost_fetch_success\": \"sum\",\n",
    "                \"time_s\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .round(6)\n",
    "    )\n",
    "\n",
    "    markdown_lines.append(\"## 📈 BY PHASE\")\n",
    "    markdown_lines.append(\"\")\n",
    "    for phase, row in phase_summary.iterrows():\n",
    "        markdown_lines.append(f\"### {phase}\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(\n",
    "            f\"- **Tokens:** {row['input_tokens']:,} input + {row['output_tokens']:,} output = {row['total_tokens']:,} total\"\n",
    "        )\n",
    "        markdown_lines.append(f\"- **Actual cost:** ${row['actual_cost_usd']:.6f}\")\n",
    "        markdown_lines.append(f\"- **Successful cost fetches:** {row['cost_fetch_success']:.0f}\")\n",
    "        markdown_lines.append(\"\")\n",
    "\n",
    "    # By model breakdown\n",
    "    model_summary = (\n",
    "        summary_df.groupby([\"model_name\"])\n",
    "        .agg(\n",
    "            {\n",
    "                \"input_tokens\": \"sum\",\n",
    "                \"output_tokens\": \"sum\",\n",
    "                \"total_tokens\": \"sum\",\n",
    "                \"actual_cost_usd\": \"sum\",\n",
    "                \"cost_fetch_success\": \"sum\",\n",
    "                \"time_s\": \"sum\",\n",
    "            }\n",
    "        )\n",
    "        .round(6)\n",
    "        .sort_values(\"total_tokens\", ascending=False)\n",
    "    )\n",
    "\n",
    "    markdown_lines.append(\"## 🤖 BY MODEL (ranked by total tokens)\")\n",
    "    markdown_lines.append(\"\")\n",
    "    for model, row in model_summary.iterrows():\n",
    "        markdown_lines.append(f\"### {model}\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(\n",
    "            f\"- **Tokens:** {row['input_tokens']:,} input + {row['output_tokens']:,} output = {row['total_tokens']:,} total\"\n",
    "        )\n",
    "        markdown_lines.append(f\"- **Actual cost:** ${row['actual_cost_usd']:.6f}\")\n",
    "        markdown_lines.append(f\"- **Successful cost fetches:** {row['cost_fetch_success']:.0f}\")\n",
    "        markdown_lines.append(\"\")\n",
    "\n",
    "    # Cost efficiency analysis\n",
    "    markdown_lines.append(\"## 💰 COST EFFICIENCY\")\n",
    "    markdown_lines.append(\"\")\n",
    "    if total_tokens > 0:\n",
    "        cost_per_1k_tokens = (total_cost / total_tokens) * 1000\n",
    "        markdown_lines.append(f\"- **Average cost per 1K tokens:** ${cost_per_1k_tokens:.6f}\")\n",
    "\n",
    "    if len(model_summary) > 1:\n",
    "        most_expensive = model_summary[\"actual_cost_usd\"].idxmax()\n",
    "        least_expensive = model_summary[\"actual_cost_usd\"].idxmin()\n",
    "        markdown_lines.append(\n",
    "            f\"- **Most expensive model:** {most_expensive} (${model_summary.loc[most_expensive, 'actual_cost_usd']:.6f})\"\n",
    "        )\n",
    "        markdown_lines.append(\n",
    "            f\"- **Least expensive model:** {least_expensive} (${model_summary.loc[least_expensive, 'actual_cost_usd']:.6f})\"\n",
    "        )\n",
    "\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(\"## 📋 Notes\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(\"- Using actual costs from OpenRouter's generation endpoint\")\n",
    "    markdown_lines.append(\n",
    "        f\"- Successful cost fetches: {successful_fetches}/{total_calls} calls ({successful_fetches/total_calls*100:.1f}%)\"\n",
    "    )\n",
    "    if successful_fetches < total_calls:\n",
    "        failed_calls = total_calls - successful_fetches\n",
    "        markdown_lines.append(f\"- {failed_calls} calls used fallback estimates\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_actual_cost(generation_id: str, client: Any) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch actual cost and token information from OpenRouter's generation endpoint.\n",
    "    \n",
    "    Args:\n",
    "        generation_id: The unique generation ID from the completion response\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing actual cost and token data, or fallback values if error\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Wait a moment for the generation to be processed by OpenRouter\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    try:\n",
    "        # Extract API key and base URL from client\n",
    "        api_key = client.api_key\n",
    "        base_url = str(client.base_url).rstrip('/') if hasattr(client, 'base_url') else \"https://openrouter.ai/api/v1\"\n",
    "        \n",
    "        # Make direct request to generation endpoint\n",
    "        import requests\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(\n",
    "            f\"{base_url}/generation?id={generation_id}\",\n",
    "            headers=headers,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            gen_data = data.get('data', {})\n",
    "            \n",
    "            return {\n",
    "                \"generation_id\": generation_id,\n",
    "                \"actual_cost_usd\": gen_data.get('total_cost', 0.0),\n",
    "                \"actual_input_tokens\": gen_data.get('native_tokens_prompt', 0),\n",
    "                \"actual_output_tokens\": gen_data.get('native_tokens_completion', 0),\n",
    "                \"actual_total_tokens\": (gen_data.get('native_tokens_prompt', 0) +\n",
    "                                     gen_data.get('native_tokens_completion', 0)),\n",
    "                \"provider_name\": gen_data.get('provider_name'),\n",
    "                \"model\": gen_data.get('model'),\n",
    "                \"latency\": gen_data.get('latency'),\n",
    "                \"generation_time\": gen_data.get('generation_time'),\n",
    "                \"cache_discount\": gen_data.get('cache_discount', 0.0),\n",
    "                \"finish_reason\": gen_data.get('finish_reason'),\n",
    "                \"fetch_success\": True\n",
    "            }\n",
    "        else:\n",
    "            print(f\"⚠️ Failed to fetch generation data for {generation_id}: HTTP {response.status_code}\")\n",
    "            return {\n",
    "                \"generation_id\": generation_id,\n",
    "                \"actual_cost_usd\": 0.0,\n",
    "                \"actual_input_tokens\": 0,\n",
    "                \"actual_output_tokens\": 0,\n",
    "                \"actual_total_tokens\": 0,\n",
    "                \"provider_name\": None,\n",
    "                \"model\": None,\n",
    "                \"latency\": None,\n",
    "                \"generation_time\": None,\n",
    "                \"cache_discount\": 0.0,\n",
    "                \"finish_reason\": None,\n",
    "                \"fetch_success\": False,\n",
    "                \"error\": f\"HTTP {response.status_code}\"\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching generation data for {generation_id}: {e}\")\n",
    "        return {\n",
    "            \"generation_id\": generation_id,\n",
    "            \"actual_cost_usd\": 0.0,\n",
    "            \"actual_input_tokens\": 0,\n",
    "            \"actual_output_tokens\": 0,\n",
    "            \"actual_total_tokens\": 0,\n",
    "            \"provider_name\": None,\n",
    "            \"model\": None,\n",
    "            \"latency\": None,\n",
    "            \"generation_time\": None,\n",
    "            \"cache_discount\": 0.0,\n",
    "            \"finish_reason\": None,\n",
    "            \"fetch_success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def fetch_all_actual_costs(df: pd.DataFrame, client: Any, generation_id_col: str = \"generation_id\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch actual costs for all generation IDs in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing generation_id column\n",
    "        client: OpenRouter API client\n",
    "        generation_id_col: Name of the column containing generation IDs\n",
    "        \n",
    "    Returns:\n",
    "        Updated DataFrame with actual cost columns populated\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\n🔍 Fetching actual costs for {len(df)} generations...\")\n",
    "    \n",
    "    # Get unique generation IDs that are not None\n",
    "    generation_ids = df[generation_id_col].dropna().unique()\n",
    "    \n",
    "    if len(generation_ids) == 0:\n",
    "        print(\"⚠️ No generation IDs found to fetch costs for\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"  Found {len(generation_ids)} unique generation IDs to fetch\")\n",
    "    \n",
    "    # Add a small delay to allow OpenRouter to process all generations\n",
    "    print(\"  Waiting 2 seconds for OpenRouter to process generations...\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Fetch costs for each unique generation ID\n",
    "    cost_data_map = {}\n",
    "    for i, gen_id in enumerate(generation_ids, 1):\n",
    "        print(f\"  [{i}/{len(generation_ids)}] Fetching cost for {gen_id}...\")\n",
    "        cost_data = fetch_actual_cost(gen_id, client)\n",
    "        cost_data_map[gen_id] = cost_data\n",
    "    \n",
    "    # Update DataFrame with fetched cost data\n",
    "    for col in [\"actual_cost_usd\", \"actual_input_tokens\", \"actual_output_tokens\",\n",
    "                \"actual_total_tokens\", \"provider_name\", \"latency\", \"generation_time\",\n",
    "                \"cache_discount\", \"finish_reason\", \"cost_fetch_success\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Map the fetched data back to the DataFrame\n",
    "    for idx, row in df.iterrows():\n",
    "        gen_id = row[generation_id_col]\n",
    "        if pd.notna(gen_id) and gen_id in cost_data_map:\n",
    "            cost_data = cost_data_map[gen_id]\n",
    "            df.at[idx, \"actual_cost_usd\"] = cost_data[\"actual_cost_usd\"]\n",
    "            df.at[idx, \"actual_input_tokens\"] = cost_data[\"actual_input_tokens\"]\n",
    "            df.at[idx, \"actual_output_tokens\"] = cost_data[\"actual_output_tokens\"]\n",
    "            df.at[idx, \"actual_total_tokens\"] = cost_data[\"actual_total_tokens\"]\n",
    "            df.at[idx, \"provider_name\"] = cost_data[\"provider_name\"]\n",
    "            df.at[idx, \"latency\"] = cost_data[\"latency\"]\n",
    "            df.at[idx, \"generation_time\"] = cost_data[\"generation_time\"]\n",
    "            df.at[idx, \"cache_discount\"] = cost_data[\"cache_discount\"]\n",
    "            df.at[idx, \"finish_reason\"] = cost_data[\"finish_reason\"]\n",
    "            df.at[idx, \"cost_fetch_success\"] = cost_data[\"fetch_success\"]\n",
    "    \n",
    "    # Report success stats\n",
    "    successful_fetches = df[\"cost_fetch_success\"].sum() if \"cost_fetch_success\" in df.columns else 0\n",
    "    total_cost = df[\"actual_cost_usd\"].sum() if \"actual_cost_usd\" in df.columns else 0\n",
    "    \n",
    "    print(\"\\n✓ Cost fetch complete:\")\n",
    "    print(f\"  Successfully fetched: {successful_fetches}/{len(df)} rows\")\n",
    "    print(f\"  Total actual cost: ${total_cost:.6f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_cost_summary(\n",
    "    questions_df: pd.DataFrame, answers_df: pd.DataFrame, ratings_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive cost and token usage summary across all API calls.\n",
    "\n",
    "    Args:\n",
    "        questions_df: DataFrame with question generation data\n",
    "        answers_df: DataFrame with answer generation data\n",
    "        ratings_df: DataFrame with rating/evaluation data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with detailed cost and token breakdowns by phase and model\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    def process_df(df, phase, model_id_col, model_name_col, time_col):\n",
    "        if \"actual_total_tokens\" in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                if row[\"actual_total_tokens\"] > 0:  # Skip error rows\n",
    "                    \n",
    "                    latency = round(row.get(\"latency\") / 1000, 2) if row.get(\"latency\") is not None else None\n",
    "                    generation_time = round(row.get(\"generation_time\") / 1000, 2) if row.get(\"generation_time\") is not None else None\n",
    "                    \n",
    "                    processing_time = None\n",
    "                    if latency is not None and generation_time is not None:\n",
    "                        processing_time = round((latency + generation_time), 2)\n",
    "                        \n",
    "                    summary_data.append(\n",
    "                        {\n",
    "                            \"phase\": phase,\n",
    "                            \"model_id\": row[model_id_col],\n",
    "                            \"model_name\": row[model_name_col],\n",
    "                            \"input_tokens\": row[\"actual_input_tokens\"],\n",
    "                            \"output_tokens\": row[\"actual_output_tokens\"],\n",
    "                            \"total_tokens\": row[\"actual_total_tokens\"],\n",
    "                            \"actual_cost_usd\": row[\"actual_cost_usd\"],\n",
    "                            \"cost_fetch_success\": row[\"cost_fetch_success\"],\n",
    "                            \"generation_id\": row.get(\"generation_id\"),\n",
    "                            \"time_s\": row[time_col],\n",
    "                            \"provider_name\": row.get(\"provider_name\"),\n",
    "                            \"latency\": latency,\n",
    "                            \"generation_time\": generation_time,\n",
    "                            \"processing_time\": processing_time,\n",
    "                            \"cache_discount\": row.get(\"cache_discount\"),\n",
    "                            \"finish_reason\": row.get(\"finish_reason\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    process_df(questions_df, \"Question Generation\", \"question_model_id\", \"question_model_name\", \"gen_time_s\")\n",
    "    process_df(answers_df, \"Answer Generation\", \"answer_model_id\", \"answer_model_name\", \"answer_time_s\")\n",
    "    process_df(ratings_df, \"Answer Rating\", \"evaluator_model_id\", \"evaluator_model_name\", \"evaluation_time_s\")\n",
    "\n",
    "    if not summary_data:\n",
    "        print(\"⚠️ No token data found in any DataFrames\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(models: pd.DataFrame, client: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate evaluation questions using the provided models.\n",
    "\n",
    "    Each model generates one challenging question designed to test reasoning depth.\n",
    "\n",
    "    Args:\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with generated questions and metadata\n",
    "\n",
    "    Example\n",
    "        >>> all_q = generate_questions(top_models, client)\n",
    "        >>> print(f\"Generated {len(all_q)} valid questions out of {len(models)} attempts\")\n",
    "        Generated 5 valid questions out of 5 attempts\n",
    "\n",
    "    Notes:\n",
    "        - Uses temperature=0.8 for creative/diverse questions\n",
    "        - Each question is limited to 10000 tokens\n",
    "        - Errors are captured in the DataFrame but don't stop execution\n",
    "        - Preview of each question is printed during generation\n",
    "        - Stores generation IDs for later cost fetching\n",
    "        - Call fetch_all_actual_costs() after generation to get actual cost data\n",
    "    \"\"\"\n",
    "    question_generation_prompt = (\n",
    "        \"Please craft ONE challenging, original, nuanced question that can effectively \"\n",
    "        \"discriminate between language models of varying reasoning depth. The question should: \"\n",
    "        \"(1) require multi-step reasoning, (2) avoid simple trivia, (3) be answerable without external \"\n",
    "        \"browsing, (4) not be purely opinion, (5) allow partial credit, and (6) be less than 400 tokens. Provide only the question text.\"\n",
    "    )\n",
    "\n",
    "    generated_questions: list[dict[str, Any]] = []\n",
    "\n",
    "    # Each model generates one question\n",
    "    for _, model in models.iterrows():\n",
    "        mid = model[\"model_id\"]\n",
    "        mname = model.get(\"model_name\", mid)\n",
    "        print(f\"\\n[Generation] {mname} generating a question...\")\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            completion = client.chat.completions.create(\n",
    "                model=mid,\n",
    "                messages=[{\"role\": \"user\", \"content\": question_generation_prompt}],\n",
    "                max_tokens=10000,  # Limit response length (increased to 10,000 for Gemini 2.5 Pro)\n",
    "                temperature=0.8,  # Higher temperature for creative/diverse questions\n",
    "            )\n",
    "            q_text = completion.choices[0].message.content.strip()\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            # Extract basic token usage from completion response\n",
    "            usage = completion.usage\n",
    "            input_tokens = usage.prompt_tokens if usage else 0\n",
    "            output_tokens = usage.completion_tokens if usage else 0\n",
    "            total_tokens = usage.total_tokens if usage else 0\n",
    "\n",
    "            # Get generation ID for later cost fetching\n",
    "            generation_id = completion.id if hasattr(completion, 'id') else None\n",
    "            \n",
    "            generated_questions.append(\n",
    "                {\n",
    "                    \"question_model_id\": mid,\n",
    "                    \"question_model_name\": mname,\n",
    "                    \"question\": q_text,\n",
    "                    \"gen_time_s\": round(elapsed, 2),\n",
    "                    \"generation_id\": generation_id,\n",
    "                    # Token counts from completion response (will be verified against actual later)\n",
    "                    \"input_tokens\": input_tokens,\n",
    "                    \"output_tokens\": output_tokens,\n",
    "                    \"total_tokens\": total_tokens,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ Question from {mname}: {q_text[:110]}{'...' if len(q_text) > 110 else ''}\")\n",
    "            print(f\"  📊 Reported tokens: {input_tokens} input + {output_tokens} output = {total_tokens} total\")\n",
    "            print(f\"  🔑 Generation ID: {generation_id}\")\n",
    "            \n",
    "        except Exception as err:  # noqa: BLE001\n",
    "            print(f\"✗ {mname} failed: {err}\")\n",
    "\n",
    "    questions_df = pd.DataFrame(generated_questions)\n",
    "\n",
    "    print(f\"\\n✓ Generated {len(questions_df)} valid questions out of {len(models)} attempts\")\n",
    "    print(\"  💡 Use fetch_all_actual_costs(questions_df, client) to retrieve actual cost data\")\n",
    "\n",
    "    return questions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d4d33",
   "metadata": {},
   "source": [
    "### 💰 Cost Tracking Pattern\n",
    "\n",
    "**New workflow for fetching actual costs:**\n",
    "\n",
    "1. **Generate content** (questions/answers/evaluations) - stores `generation_id` in DataFrame\n",
    "2. **Wait briefly** - allows OpenRouter to process all generations  \n",
    "3. **Fetch costs in batch** - use `fetch_all_actual_costs(df, client)` to retrieve actual cost data\n",
    "\n",
    "**Benefits:**\n",
    "- ✅ Avoids immediate API calls that may return incomplete data\n",
    "- ✅ Batch processing is more efficient \n",
    "- ✅ Separates content generation from cost tracking\n",
    "- ✅ Can re-run cost fetching if needed without regenerating content\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Step 1: Generate questions (stores generation_id)\n",
    "questions_df = generate_questions(top_models, client)\n",
    "\n",
    "# Step 2: Fetch actual costs (updates DataFrame with cost data)\n",
    "questions_df_full = fetch_all_actual_costs(questions_df, client)\n",
    "\n",
    "# Now questions_df_full has columns: actual_cost_usd, actual_input_tokens, \n",
    "# actual_output_tokens, provider_name, latency, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df = generate_questions(top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch actual costs for all generated questions\n",
    "# OpenRouter has a slight delay before generation data is available\n",
    "questions_df_full = fetch_all_actual_costs(questions_df, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22686585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(questions_df: pd.DataFrame, models: pd.DataFrame, client: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate answers to questions using the provided models.\n",
    "\n",
    "    Each model answers every question from every model (including its own question).\n",
    "\n",
    "    Args:\n",
    "        questions_df: DataFrame with questions (must have 'question' and 'question_model_name' columns)\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with answers containing columns: question_model_name, question,\n",
    "        answer_model_id, answer_model_name, answer, answer_time_s, generation_id, etc.\n",
    "        \n",
    "    Notes:\n",
    "        - Stores generation IDs for later cost fetching\n",
    "        - Call fetch_all_actual_costs() after generation to get actual cost data\n",
    "    \"\"\"\n",
    "    answers: list[dict[str, Any]] = []\n",
    "    answer_instructions = (\n",
    "        \"You will be given a question designed to evaluate reasoning depth. Provide a thorough, \"\n",
    "        \"structured answer. Show reasoning explicitly if helpful, but keep it concise and logical.\"\n",
    "        \"If you have to iterate more than 5 times, admit you are stuck and provide your best possible answer.\"\n",
    "    )\n",
    "\n",
    "    # Each model answers each question\n",
    "    for _, qrow in questions_df.iterrows():\n",
    "        q_text = qrow[\"question\"]\n",
    "        origin_model = qrow[\"question_model_name\"]\n",
    "\n",
    "        for _, model in models.iterrows():\n",
    "            mid = model[\"model_id\"]\n",
    "            mname = model.get(\"model_name\", mid)\n",
    "            print(f\"\\n[Answer] {mname} answering question from {origin_model}...\")\n",
    "\n",
    "            try:\n",
    "                start = time.time()\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=mid,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": answer_instructions},\n",
    "                        {\"role\": \"user\", \"content\": q_text},\n",
    "                    ],\n",
    "                    max_tokens=20000,  # Allow longer responses for thorough answers (especially for Gemini 2.5 Pro)\n",
    "                    temperature=0.5,  # Moderate temperature for balanced responses\n",
    "                    timeout=30.0,  # 30 second timeout to prevent hanging\n",
    "                )\n",
    "                ans_text = completion.choices[0].message.content.strip()\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "                # Extract basic token usage from completion response\n",
    "                usage = completion.usage\n",
    "                input_tokens = usage.prompt_tokens if usage else 0\n",
    "                output_tokens = usage.completion_tokens if usage else 0\n",
    "                total_tokens = usage.total_tokens if usage else 0\n",
    "                \n",
    "                # Get generation ID for later cost fetching\n",
    "                generation_id = completion.id if hasattr(completion, 'id') else None\n",
    "                \n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"question_model_name\": origin_model,\n",
    "                        \"question\": q_text,\n",
    "                        \"answer_model_id\": mid,\n",
    "                        \"answer_model_name\": mname,\n",
    "                        \"answer\": ans_text,\n",
    "                        \"answer_time_s\": round(elapsed, 2),\n",
    "                        \"generation_id\": generation_id,\n",
    "                        # Token counts from completion response (will be verified against actual later)\n",
    "                        \"input_tokens\": input_tokens,\n",
    "                        \"output_tokens\": output_tokens,\n",
    "                        \"total_tokens\": total_tokens,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                print(f\"✓ Answer length: {len(ans_text)} chars\")\n",
    "                print(f\"  📊 Reported tokens: {input_tokens} input + {output_tokens} output = {total_tokens} total\")\n",
    "                print(f\"  🔑 Generation ID: {generation_id}\")\n",
    "                    \n",
    "            except Exception as err:  # noqa: BLE001\n",
    "                # Record errors but continue with other models\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"question_model_name\": origin_model,\n",
    "                        \"question\": q_text,\n",
    "                        \"answer_model_id\": mid,\n",
    "                        \"answer_model_name\": mname,\n",
    "                        \"answer\": f\"Error: {err}\",\n",
    "                        \"answer_time_s\": None,\n",
    "                        \"generation_id\": None,\n",
    "                        \"input_tokens\": 0,\n",
    "                        \"output_tokens\": 0,\n",
    "                        \"total_tokens\": 0,\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✗ {mname} failed to answer: {err}\")\n",
    "\n",
    "    answers_df = pd.DataFrame(answers)\n",
    "    \n",
    "    print(f\"\\n✓ Answers DataFrame ready with {len(answers_df)} answers\")\n",
    "    print(\"  💡 Use fetch_all_actual_costs(answers_df, client) to retrieve actual cost data\")\n",
    "\n",
    "    return answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eebcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = answer_questions(questions_df_full, top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch actual costs for all generated answers\n",
    "answers_df_full = fetch_all_actual_costs(answers_df, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check various answers, just change the index number (0 based )\n",
    "print(answers_df_full.iloc[23][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154383b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(answers_df: pd.DataFrame, models: pd.DataFrame, client: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate ratings for answers using the evaluation models.\n",
    "\n",
    "    Each model rates every answer given by every model (including its own).\n",
    "\n",
    "    Args:\n",
    "        answers_df: DataFrame with answers (must have columns for question, answer, etc.)\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with evaluations containing columns: question_model_name, question,\n",
    "        answer_model_name, answer, evaluator_model_id, evaluator_model_name, rating,\n",
    "        explanation, evaluation_time_s, generation_id, etc.\n",
    "        \n",
    "    Notes:\n",
    "        - Stores generation IDs for later cost fetching\n",
    "        - Call fetch_all_actual_costs() after generation to get actual cost data\n",
    "    \"\"\"\n",
    "    evaluations: list[dict[str, Any]] = []\n",
    "    # rating_instructions = (\n",
    "    #     \"You are evaluating the quality of an answer to a reasoning-focused question. \"\n",
    "    #     \"Rate the answer on a scale from 1-5 (integers only, 5 = outstanding).\\n\"\n",
    "    #     \"1 = Poor (incorrect, unhelpful, or off-topic)\\n\"\n",
    "    #     \"2 = Below average (partially correct but significant issues)\\n\"\n",
    "    #     \"3 = Average (mostly correct but lacking depth or clarity)\\n\"\n",
    "    #     \"4 = Good (correct, clear, and helpful)\\n\"\n",
    "    #     \"5 = Excellent (exceptional quality, comprehensive, insightful)\\n\\n\"\n",
    "    #     \"Provide ONLY the numeric rating (1-5) followed by a brief explanation. \"\n",
    "    #     \"Format: Rating: X\\nExplanation: [your reasoning]\"\n",
    "    # )\n",
    "\n",
    "    rating_instructions = (\n",
    "        \"You are evaluating the quality of an answer to a reasoning-focused question. \"\n",
    "        \"Rate the answer on a scale from 1-10 (integers only, 10 = outstanding).\\n\"\n",
    "        \"Criteria (roughly equal weight):\\n\"\n",
    "        \"1. Clarity & organization\\n\"\n",
    "        \"2. Depth & correctness\\n\"\n",
    "        \"3. Completeness\\n\"\n",
    "        \"4. Insight/originality (if applicable)\\n\\n\"\n",
    "        \"Provide ONLY the numeric rating (1-10) followed by a brief explanation.\\n\"\n",
    "        \"Format: Rating: X\\nExplanation: [your reasoning]\"\n",
    "    )\n",
    "\n",
    "    # Each model evaluates each answer\n",
    "    for _, ans_row in answers_df.iterrows():\n",
    "        question = ans_row[\"question\"]\n",
    "        answer = ans_row[\"answer\"]\n",
    "        q_model = ans_row[\"question_model_name\"]\n",
    "        a_model = ans_row[\"answer_model_name\"]\n",
    "\n",
    "        for _, model in models.iterrows():\n",
    "            mid = model[\"model_id\"]\n",
    "            mname = model.get(\"model_name\", mid)\n",
    "            print(f\"\\n[Evaluate] {mname} rating answer from {a_model} to {q_model}'s question...\")\n",
    "\n",
    "            try:\n",
    "                eval_prompt = f\"Question: {question}\\n\\nAnswer: {answer}\"\n",
    "                \n",
    "                start = time.time()\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=mid,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": rating_instructions},\n",
    "                        {\"role\": \"user\", \"content\": eval_prompt},\n",
    "                    ],\n",
    "                    max_tokens=10000,  # Shorter responses for evaluations\n",
    "                    temperature=0.1,  # Low temperature for consistent evaluations\n",
    "                    timeout=30.0,  # 30 second timeout to prevent hanging\n",
    "                )\n",
    "                eval_text = completion.choices[0].message.content.strip()\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "                # Extract rating from the response\n",
    "                rating = None\n",
    "                explanation = eval_text\n",
    "                try:\n",
    "                    if \"Rating:\" in eval_text:\n",
    "                        parts = eval_text.split(\"Rating:\", 1)[1].split(\"\\n\", 1)\n",
    "                        rating_str = parts[0].strip()\n",
    "                        rating = int(rating_str)\n",
    "                        if len(parts) > 1 and \"Explanation:\" in parts[1]:\n",
    "                            explanation = parts[1].split(\"Explanation:\", 1)[1].strip()\n",
    "                    else:\n",
    "                        # Try to extract number from beginning\n",
    "                        import re\n",
    "                        match = re.search(r'\\b([1-5])\\b', eval_text)\n",
    "                        if match:\n",
    "                            rating = int(match.group(1))\n",
    "                except (ValueError, IndexError):\n",
    "                    rating = None  # Will be handled as parsing error\n",
    "\n",
    "                # Extract basic token usage from completion response\n",
    "                usage = completion.usage\n",
    "                input_tokens = usage.prompt_tokens if usage else 0\n",
    "                output_tokens = usage.completion_tokens if usage else 0\n",
    "                total_tokens = usage.total_tokens if usage else 0\n",
    "                \n",
    "                # Get generation ID for later cost fetching\n",
    "                generation_id = completion.id if hasattr(completion, 'id') else None\n",
    "                \n",
    "                evaluations.append(\n",
    "                    {\n",
    "                        \"question_model_name\": q_model,\n",
    "                        \"question\": question,\n",
    "                        \"answer_model_id\": ans_row[\"answer_model_id\"],\n",
    "                        \"answer_model_name\": a_model,\n",
    "                        \"answer\": answer,\n",
    "                        \"evaluator_model_id\": mid,\n",
    "                        \"evaluator_model_name\": mname,\n",
    "                        \"rating\": rating,\n",
    "                        \"explanation\": explanation,\n",
    "                        \"full_evaluation\": eval_text,\n",
    "                        \"evaluation_time_s\": round(elapsed, 2),\n",
    "                        \"generation_id\": generation_id,\n",
    "                        # Token counts from completion response (will be verified against actual later)\n",
    "                        \"input_tokens\": input_tokens,\n",
    "                        \"output_tokens\": output_tokens,\n",
    "                        \"total_tokens\": total_tokens,\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                print(f\"✓ Rating: {rating} | Explanation length: {len(explanation)} chars\")\n",
    "                print(f\"  📊 Reported tokens: {input_tokens} input + {output_tokens} output = {total_tokens} total\")\n",
    "                print(f\"  🔑 Generation ID: {generation_id}\")\n",
    "                    \n",
    "            except Exception as err:  # noqa: BLE001\n",
    "                # Record errors but continue with other models\n",
    "                evaluations.append(\n",
    "                    {\n",
    "                        \"question_model_name\": q_model,\n",
    "                        \"question\": question,\n",
    "                        \"answer_model_id\": ans_row[\"answer_model_id\"],\n",
    "                        \"answer_model_name\": a_model,\n",
    "                        \"answer\": answer,\n",
    "                        \"evaluator_model_id\": mid,\n",
    "                        \"evaluator_model_name\": mname,\n",
    "                        \"rating\": None,\n",
    "                        \"explanation\": f\"Error: {err}\",\n",
    "                        \"full_evaluation\": f\"Error: {err}\",\n",
    "                        \"evaluation_time_s\": None,\n",
    "                        \"generation_id\": None,\n",
    "                        \"input_tokens\": 0,\n",
    "                        \"output_tokens\": 0,\n",
    "                        \"total_tokens\": 0,\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✗ {mname} failed to evaluate: {err}\")\n",
    "\n",
    "    ratings_df = pd.DataFrame(evaluations)\n",
    "    \n",
    "    print(f\"\\n✓ Ratings DataFrame ready with {len(ratings_df)} evaluations\")\n",
    "    print(\"  💡 Use fetch_all_actual_costs(ratings_df, client) to retrieve actual cost data\")\n",
    "\n",
    "    return ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25f3d9",
   "metadata": {},
   "source": [
    "### Understanding the Rating System\n",
    "\n",
    "Each model evaluates answers on a **10-point scale** across these criteria:\n",
    "\n",
    "1. **Clarity & Organization** (2.5 points)\n",
    "   - Is the answer well-structured and easy to follow?\n",
    "   - Are concepts explained clearly without unnecessary jargon?\n",
    "\n",
    "2. **Depth & Correctness** (2.5 points)\n",
    "   - Is the reasoning sound and logically valid?\n",
    "   - Are facts accurate and relevant?\n",
    "\n",
    "3. **Completeness** (2.5 points)\n",
    "   - Does the answer address all parts of the question?\n",
    "   - Are edge cases or caveats mentioned when appropriate?\n",
    "\n",
    "4. **Insight & Originality** (2.5 points)\n",
    "   - Does the answer provide novel perspectives or connections?\n",
    "   - Is there evidence of deeper understanding beyond surface-level knowledge?\n",
    "\n",
    "**Why use models as raters?**\n",
    "- Consistent evaluation criteria across all answers\n",
    "- Faster than human evaluation for large-scale comparisons\n",
    "- Tests if models can accurately judge reasoning quality (meta-evaluation)\n",
    "\n",
    "**Limitations:**\n",
    "- Models may have biases (e.g., preferring similar styles to their own)\n",
    "- Some subjective criteria may be interpreted differently\n",
    "- This is why we aggregate ratings across multiple model-raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = evaluate_answers(answers_df_full, top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48455d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch actual costs for all generated ratings\n",
    "ratings_df_full = fetch_all_actual_costs(ratings_df, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432188cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c906ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_summary_df = generate_cost_summary(questions_df_full, answers_df_full, ratings_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352941a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(generate_cost_summary_markdown(cost_summary_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(ratings_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Generate aggregation summaries\n",
    "    summary_model = pd.DataFrame()\n",
    "    summary_pair = pd.DataFrame()\n",
    "\n",
    "    if not ratings_df.empty:\n",
    "        # Calculate average rating for each model's answers\n",
    "        summary_model = (\n",
    "            ratings_df.groupby(\"answer_model_name\")[\"rating\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"rating\": \"avg_rating\"})\n",
    "        )\n",
    "        \n",
    "        # Calculate average rating for each (answerer, evaluator) pair\n",
    "        summary_pair = (\n",
    "            ratings_df.groupby([\"answer_model_name\", \"evaluator_model_name\"])[\"rating\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"rating\": \"avg_rating\"})\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Average rating per answer model:\")\n",
    "        print(\"=\"*80)\n",
    "        display(summary_model.sort_values(\"avg_rating\", ascending=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Cross-model rating matrix (long form):\")\n",
    "        print(\"=\"*80)\n",
    "        display(summary_pair.head(20))\n",
    "    else:\n",
    "        print(\"⚠ No ratings captured.\")\n",
    "\n",
    "    return summary_model, summary_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56154347",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model, summary_pair = generate_summary(ratings_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38186a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96853b26",
   "metadata": {},
   "source": [
    "### Summary Functions that generate markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pivot table to markdown format\n",
    "def pivot_to_markdown(pivot_df):\n",
    "    \"\"\"Convert a pivot table DataFrame to markdown table format.\"\"\"\n",
    "    if pivot_df.empty:\n",
    "        return \"No data available\"\n",
    "\n",
    "    # Start with header row\n",
    "    headers = [\"Model\"] + list(pivot_df.columns)\n",
    "    markdown_lines = []\n",
    "\n",
    "    # Create header\n",
    "    header_line = \"| \" + \" | \".join(headers) + \" |\"\n",
    "    markdown_lines.append(header_line)\n",
    "\n",
    "    # Create separator line\n",
    "    separator = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "    markdown_lines.append(separator)\n",
    "\n",
    "    # Add data rows\n",
    "    for index, row in pivot_df.iterrows():\n",
    "        row_data = [str(index)]\n",
    "        for col in pivot_df.columns:\n",
    "            value = row[col]\n",
    "            if pd.isna(value):\n",
    "                row_data.append(\"N/A\")\n",
    "            else:\n",
    "                row_data.append(f\"{value:.2f}\")\n",
    "        row_line = \"| \" + \" | \".join(row_data) + \" |\"\n",
    "        markdown_lines.append(row_line)\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def top_models_markdown(top_models: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a markdown table of top models with selected columns.\"\"\"\n",
    "    if top_models.empty:\n",
    "        return \"⚠️ No model data available\"\n",
    "    \n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 🏆 Top Models Selected for Evaluation\\n\")\n",
    "    markdown_lines.append(\"| Rank | Model Name | Tokens Used | Token Change |\")\n",
    "    markdown_lines.append(\"|------|------------|-------------|--------------|\")\n",
    "    \n",
    "    for _, row in top_models.iterrows():\n",
    "        rank = row['rank']\n",
    "        model_name = row['model_name']\n",
    "        tokens = row['tokens_formatted']\n",
    "        change = row['token_change']\n",
    "        markdown_lines.append(f\"| {rank} | {model_name} | {tokens} | {change} |\")\n",
    "    \n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def questions_to_markdown(questions_df):\n",
    "    \"\"\"Convert generated questions DataFrame to markdown format.\"\"\"\n",
    "    if questions_df.empty:\n",
    "        return \"No questions available\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 📝 Generated Evaluation Questions\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    for idx, row in questions_df.iterrows():\n",
    "        markdown_lines.append(f\"### {idx + 1}. Question by {row['question_model_name']}\")\n",
    "        markdown_lines.append(f\"**Generation Time:** {row['gen_time_s']}s\")\n",
    "        if \"actual_total_tokens\" in row and row[\"actual_total_tokens\"] > 0:\n",
    "            markdown_lines.append(f\"**Tokens Used:** {row['actual_input_tokens']} in + {row['actual_output_tokens']} out = {row['actual_total_tokens']} total\")\n",
    "            markdown_lines.append(f\"**Actual Cost:** ${row['actual_cost_usd']:.4f}\")\n",
    "        markdown_lines.append(f\"{row['question']}\")\n",
    "        markdown_lines.append(\"\\n\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def model_performance_summary_markdown(summary_model: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a model performance summary in markdown format.\"\"\"\n",
    "    if summary_model.empty:\n",
    "        return \"⚠️ No model performance data available\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 📊 Model Performance Summary\\n\")\n",
    "    markdown_lines.append(\"### Overall Average Ratings (10-point scale)\\n\")\n",
    "\n",
    "    for idx, row in summary_model.sort_values(\"avg_rating\", ascending=False).iterrows():\n",
    "        markdown_lines.append(f\"{idx + 1}. **{row['answer_model_name']}**: {row['avg_rating']:.2f}/10\")\n",
    "\n",
    "    # markdown_lines.append(\"### Model Stats)\\n\")\n",
    "    # for idx, row in summary_model.iterrows():\n",
    "    #     markdown_lines.append(f\"### {idx + 1}. {row['answer_model_name']}\")\n",
    "    #     markdown_lines.append(f\"- **Average Response Time:** {row['avg_response_time_s']}s\")\n",
    "    #     markdown_lines.append(f\"- **Total Tokens Used:** {row['total_tokens']}\")\n",
    "    #     markdown_lines.append(f\"- **Total Cost:** ${row['total_cost_usd']:.4f}\")\n",
    "    #     markdown_lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def cost_summary_markdown(cost_summary_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a cost summary in markdown format.\"\"\"\n",
    "    if cost_summary_df.empty:\n",
    "        return \"⚠️ No cost data available\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 💲 Cost Summary and Token Usage\\n\")\n",
    "    \n",
    "    total_cost = cost_summary_df[\"actual_cost_usd\"].sum()\n",
    "    total_tokens = cost_summary_df[\"total_tokens\"].sum()\n",
    "    input_tokens = cost_summary_df[\"input_tokens\"].sum()\n",
    "    output_tokens = cost_summary_df[\"output_tokens\"].sum()\n",
    "\n",
    "    markdown_lines.append(f\"- **Total Cost Across All Phases:** ${total_cost:.4f}\")\n",
    "    markdown_lines.append(f\"- **Total Tokens Used Across All Phases:** {total_tokens:,}\")\n",
    "    markdown_lines.append(f\"- **Total Input Tokens:** {input_tokens:,}\")\n",
    "    markdown_lines.append(f\"- **Total Output Tokens:** {output_tokens:,}\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def generate_performance_tables_markdown(cost_summary_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate performance metrics tables showing average cost, time, latency, and tokens\n",
    "    by model and phase in markdown format.\n",
    "    \n",
    "    Args:\n",
    "        cost_summary_df: DataFrame with columns: phase, model_name, actual_cost_usd, \n",
    "                        time_s, latency, generation_time, total_tokens, processing_time\n",
    "    \n",
    "    Returns:\n",
    "        Markdown string with four performance metric tables\n",
    "    \"\"\"\n",
    "    if cost_summary_df.empty:\n",
    "        return \"⚠️ No cost summary data available\"\n",
    "    \n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 📊 Performance Metrics by Model and Phase\\n\")\n",
    "    \n",
    "    # Define the metrics to generate tables for\n",
    "    metrics = [\n",
    "        {\n",
    "            \"title\": \"Average Cost ($)\",\n",
    "            \"column\": \"actual_cost_usd\",\n",
    "            \"format\": lambda x: f\"{x:.4f}\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Average Time (s)\",\n",
    "            \"column\": \"time_s\",\n",
    "            \"format\": lambda x: f\"{x:.2f}\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Average Latency + Gen (s)\",\n",
    "            \"column\": \"processing_time\",\n",
    "            \"format\": lambda x: f\"{x:.2f}\" if pd.notna(x) else \"N/A\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Average Tokens\",\n",
    "            \"column\": \"total_tokens\",\n",
    "            \"format\": lambda x: f\"{int(x)}\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Generate a table for each metric\n",
    "    for metric in metrics:\n",
    "        markdown_lines.append(f\"### {metric['title']}\\n\")\n",
    "        \n",
    "        # Pivot the data: rows = models, columns = phases\n",
    "        pivot_df = cost_summary_df.pivot_table(\n",
    "            index='model_name',\n",
    "            columns='phase',\n",
    "            values=metric['column'],\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Reorder columns to match expected order\n",
    "        phase_order = ['Question Generation', 'Answer Generation', 'Answer Rating']\n",
    "        pivot_df = pivot_df[[col for col in phase_order if col in pivot_df.columns]]\n",
    "        \n",
    "        # Calculate overall average across all phases\n",
    "        pivot_df['Overall Average'] = pivot_df.mean(axis=1)\n",
    "        \n",
    "        # Generate markdown table header\n",
    "        headers = [\"Model\"] + list(pivot_df.columns)\n",
    "        markdown_lines.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "        markdown_lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "        \n",
    "        # Generate table rows\n",
    "        for model_name, row in pivot_df.iterrows():\n",
    "            row_values = [str(model_name)]\n",
    "            for col in pivot_df.columns:\n",
    "                value = row[col]\n",
    "                if pd.notna(value):\n",
    "                    row_values.append(metric['format'](value))\n",
    "                else:\n",
    "                    row_values.append(\"N/A\")\n",
    "            markdown_lines.append(\"| \" + \" | \".join(row_values) + \" |\")\n",
    "        \n",
    "        markdown_lines.append(\"\")  # Empty line between tables\n",
    "    \n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def cross_model_rating_matrix_markdown(summary_pair: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a cross-model rating matrix in markdown format.\"\"\"\n",
    "    if summary_pair.empty:\n",
    "        return \"⚠️ No cross-model rating data available\"\n",
    "\n",
    "    # Pivot the DataFrame to create a matrix\n",
    "    pivot_df = summary_pair.pivot(index=\"answer_model_name\", columns=\"evaluator_model_name\", values=\"avg_rating\")\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 🤝 Cross-Model Rating Matrix\\n\")\n",
    "    markdown_lines.append(\"This shows how each model (rater/columns) rated each model's answers (answerer/rows):\\n\")\n",
    "    markdown_lines.append(pivot_to_markdown(pivot_df))\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def cost_breakdown_by_phase_markdown(cost_summary_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a cost breakdown by evaluation phase in markdown format.\"\"\"\n",
    "    if cost_summary_df.empty:\n",
    "        return \"⚠️ Run the full pipeline first to see cost breakdown\\n   Execute the cell: 'Run the complete evaluation pipeline with cost tracking'\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 💰 Cost Breakdown by Phase\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    for phase in [\"Question Generation\", \"Answer Generation\", \"Answer Rating\"]:\n",
    "        phase_data = cost_summary_df[cost_summary_df[\"phase\"] == phase]\n",
    "        if not phase_data.empty:\n",
    "            total_calls = len(phase_data)\n",
    "            total_tokens = phase_data[\"total_tokens\"].sum()\n",
    "            total_cost = phase_data[\"actual_cost_usd\"].sum()\n",
    "            avg_tokens_per_call = total_tokens / total_calls if total_calls > 0 else 0\n",
    "            avg_cost_per_call = total_cost / total_calls if total_calls > 0 else 0\n",
    "\n",
    "            markdown_lines.append(f\"### 📊 {phase}\\n\")\n",
    "            markdown_lines.append(f\"- **API calls**: {total_calls}\")\n",
    "            markdown_lines.append(f\"- **Total tokens**: {total_tokens:,}\")\n",
    "            markdown_lines.append(f\"- **Total cost**: ${total_cost:.4f}\")\n",
    "            markdown_lines.append(f\"- **Avg tokens/call**: {avg_tokens_per_call:.0f}\")\n",
    "            markdown_lines.append(f\"- **Avg cost/call**: ${avg_cost_per_call:.4f}\")\n",
    "            markdown_lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def most_expensive_calls_markdown(cost_summary_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate a summary of the most expensive individual API calls in markdown format.\"\"\"\n",
    "    if cost_summary_df.empty:\n",
    "        return \"⚠️ Run the full pipeline first to see expensive calls\\n   Execute the cell: 'Run the complete evaluation pipeline with cost tracking'\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## 💸 Most Expensive API Calls\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    top_expensive = cost_summary_df.nlargest(5, \"actual_cost_usd\")\n",
    "    for idx, row in top_expensive.iterrows():\n",
    "        markdown_lines.append(f\"### {idx + 1}. {row['model_name']} ({row['phase']})\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(f\"- **Tokens**: {row['input_tokens']:,} in + {row['output_tokens']:,} out = {row['total_tokens']:,}\")\n",
    "        markdown_lines.append(f\"- **Cost**: ${row['actual_cost_usd']:.4f}\")\n",
    "        markdown_lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n",
    "\n",
    "def generate_rating_statistics_markdown(ratings_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate rating statistics in markdown format.\"\"\"\n",
    "    if ratings_df.empty or \"rating\" not in ratings_df.columns:\n",
    "        return \"⚠️ **No valid ratings data available**\"\n",
    "\n",
    "    # Overall statistics\n",
    "    valid_ratings = ratings_df[\"rating\"].dropna()\n",
    "\n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## Rating Statistics Summary\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    markdown_lines.append(\"### 📊 Overall Rating Distribution\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(f\"- **Total ratings collected**: {len(valid_ratings)}\")\n",
    "    markdown_lines.append(f\"- **Average rating**: {valid_ratings.mean():.2f} / 10\")\n",
    "    markdown_lines.append(f\"- **Median rating**: {valid_ratings.median():.1f} / 10\")\n",
    "    markdown_lines.append(f\"- **Standard deviation**: {valid_ratings.std():.2f}\")\n",
    "    markdown_lines.append(f\"- **Range**: {valid_ratings.min():.0f} - {valid_ratings.max():.0f}\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    # Rating distribution\n",
    "    markdown_lines.append(\"### 📈 Rating Frequency\")\n",
    "    markdown_lines.append(\"\")\n",
    "    rating_counts = valid_ratings.value_counts().sort_index(ascending=False)\n",
    "    for score, count in rating_counts.items():\n",
    "        bar = \"█\" * int(count / len(valid_ratings) * 20)  # Shorter bars for markdown\n",
    "        percentage = count / len(valid_ratings) * 100\n",
    "        markdown_lines.append(f\"- **{score:2.0f}/10**: `{bar}` ({count} ratings, {percentage:.1f}%)\")\n",
    "    markdown_lines.append(\"\")\n",
    "\n",
    "    # Self-rating analysis (do models rate themselves higher?)\n",
    "    if \"answer_model_name\" in ratings_df.columns and \"rater_model_name\" in ratings_df.columns:\n",
    "        self_ratings = ratings_df[ratings_df[\"answer_model_name\"] == ratings_df[\"rater_model_name\"]][\n",
    "            \"rating\"\n",
    "        ].dropna()\n",
    "\n",
    "        other_ratings = ratings_df[ratings_df[\"answer_model_name\"] != ratings_df[\"rater_model_name\"]][\n",
    "            \"rating\"\n",
    "        ].dropna()\n",
    "\n",
    "        if len(self_ratings) > 0 and len(other_ratings) > 0:\n",
    "            markdown_lines.append(\"### 🤔 Self-Rating vs. Peer-Rating\")\n",
    "            markdown_lines.append(\"\")\n",
    "            markdown_lines.append(f\"- **Average when rating own answers**: {self_ratings.mean():.2f}\")\n",
    "            markdown_lines.append(f\"- **Average when rating others' answers**: {other_ratings.mean():.2f}\")\n",
    "            diff = self_ratings.mean() - other_ratings.mean()\n",
    "\n",
    "            if abs(diff) < 0.5:\n",
    "                bias_text = f\"Models are fairly unbiased (difference: {diff:+.2f})\"\n",
    "            elif diff > 0:\n",
    "                bias_text = f\"Models tend to rate themselves higher (difference: {diff:+.2f})\"\n",
    "            else:\n",
    "                bias_text = f\"Models tend to rate themselves lower (difference: {diff:+.2f})\"\n",
    "\n",
    "            markdown_lines.append(f\"- **Bias Analysis**: {bias_text}\")\n",
    "            markdown_lines.append(\"\")\n",
    "\n",
    "    # Most generous and strictest raters\n",
    "    if \"rater_model_name\" in ratings_df.columns:\n",
    "        avg_by_rater = ratings_df.groupby(\"rater_model_name\")[\"rating\"].mean().sort_values(ascending=False)\n",
    "\n",
    "        markdown_lines.append(\"### 🎭 Rater Characteristics\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(\n",
    "            f\"- **🎁 Most Generous Rater**: {avg_by_rater.index[0]} (avg: {avg_by_rater.iloc[0]:.2f})\"\n",
    "        )\n",
    "        markdown_lines.append(f\"- **🔍 Strictest Rater**: {avg_by_rater.index[-1]} (avg: {avg_by_rater.iloc[-1]:.2f})\")\n",
    "        markdown_lines.append(f\"- **📏 Rater Spread**: {avg_by_rater.iloc[0] - avg_by_rater.iloc[-1]:.2f} points\")\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4541ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_rated_answers(ratings_df: pd.DataFrame, cost_summary_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the highest rated answer for each question.\n",
    "    In case of a tie, select the cheapest one based on actual_cost_usd.\n",
    "    \n",
    "    Args:\n",
    "        ratings_df: DataFrame with ratings data\n",
    "        cost_summary_df: DataFrame with cost data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with the highest rated answer for each question\n",
    "    \"\"\"\n",
    "    # Merge ratings with cost data to get actual_cost_usd for each answer\n",
    "    # We need to match on the answer generation phase\n",
    "    answer_costs = cost_summary_df[cost_summary_df['phase'] == 'Answer Generation'].copy()\n",
    "    \n",
    "    # Merge on answer_model_name (from ratings_df) with model_name (from cost_summary_df)\n",
    "    ratings_with_cost = ratings_df.merge(\n",
    "        answer_costs[['model_name', 'actual_cost_usd']],\n",
    "        left_on='answer_model_name',\n",
    "        right_on='model_name',\n",
    "        how='left',\n",
    "        suffixes=('', '_answer_cost')\n",
    "    )\n",
    "    \n",
    "    # For each question, get the average rating per answer model\n",
    "    # Preserve answer, explanation, and full_evaluation columns using 'first'\n",
    "    # For evaluator info, we'll take first evaluator and sum evaluation times\n",
    "    avg_ratings = ratings_with_cost.groupby(['question_model_name', 'question', 'answer_model_name']).agg({\n",
    "        'rating': 'mean',\n",
    "        'actual_cost_usd': 'first',  # Take first value since it's the same for all ratings of same answer\n",
    "        'answer': 'first',  # These are the same for all ratings of the same answer\n",
    "        'explanation': 'first',\n",
    "        'full_evaluation': 'first',\n",
    "        'evaluator_model_name': 'first',\n",
    "        'evaluation_time_s': 'first'\n",
    "        # 'evaluator_model_name': lambda x: ', '.join(sorted(set(x))),  # List all unique evaluators\n",
    "        # 'evaluation_time_s': 'sum'  # Sum of all evaluation times for this answer\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by rating (descending) and cost (ascending) to handle ties\n",
    "    avg_ratings['actual_cost_usd'] = pd.to_numeric(avg_ratings['actual_cost_usd'], errors='coerce')\n",
    "    avg_ratings = avg_ratings.sort_values(\n",
    "        ['question_model_name', 'rating', 'actual_cost_usd'],\n",
    "        ascending=[True, False, True]\n",
    "    )\n",
    "    \n",
    "    # Get the top answer for each question\n",
    "    highest_rated = avg_ratings.groupby('question_model_name').first().reset_index()\n",
    "    \n",
    "    # Add rank to show position\n",
    "    highest_rated['rank'] = range(1, len(highest_rated) + 1)\n",
    "    \n",
    "    return highest_rated[['rank', 'question_model_name', 'answer_model_name', 'rating', 'actual_cost_usd', 'question', 'answer', 'explanation', 'full_evaluation', 'evaluator_model_name', 'evaluation_time_s']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93864188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_evaluation_markdown(question_model_name: str, answer_model_name: str, ratings_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a markdown-formatted summary of answer evaluations.\n",
    "    \n",
    "    Shows the question and answer once at the top, followed by individual\n",
    "    evaluations from each rater model. Selects evaluations for the specified\n",
    "    question and answer models.\n",
    "    \n",
    "    Args:\n",
    "        question_model_name: Name of the question model\n",
    "        answer_model_name: Name of the answer model\n",
    "        ratings_df: DataFrame with evaluation data (typically filtered to one question/answer pair)\n",
    "    Returns:\n",
    "        Markdown-formatted string\n",
    "    \"\"\"\n",
    "    df = ratings_df[\n",
    "        (ratings_df['question_model_name'] == question_model_name) &\n",
    "        (ratings_df['answer_model_name'] == answer_model_name)\n",
    "    ]\n",
    "\n",
    "    if df.empty:\n",
    "        return \"⚠️ No evaluation data available\"\n",
    "    \n",
    "    markdown_lines = []\n",
    "    \n",
    "    # Get common values (should be the same across all rows)\n",
    "    first_row = df.iloc[0]\n",
    "    question_model = first_row['question_model_name']\n",
    "    question = first_row['question']\n",
    "    answer_model = first_row['answer_model_name']\n",
    "    answer = first_row['answer']\n",
    "    \n",
    "    # Header\n",
    "    markdown_lines.append(\"## Answer Evaluation Summary\")\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Question section\n",
    "    markdown_lines.append(\"## 📝 Question\\n\")\n",
    "    markdown_lines.append(f\"**Asked by:** {question_model}\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(question)\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Answer section\n",
    "    markdown_lines.append(\"## 💡 Answer\\n\")\n",
    "    markdown_lines.append(f\"**Answered by:** {answer_model}\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(answer)\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Evaluations section\n",
    "    markdown_lines.append(\"## 🎯 Evaluations\")\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Sort by rating (highest first) for easier reading\n",
    "    sorted_df = df.sort_values('rating', ascending=False)\n",
    "    \n",
    "    for _, row in sorted_df.iterrows():\n",
    "        evaluator = row['evaluator_model_name']\n",
    "        rating = row['rating']\n",
    "        explanation = row['explanation']\n",
    "        full_eval = row['full_evaluation']\n",
    "        eval_time = row['evaluation_time_s']\n",
    "        \n",
    "        markdown_lines.append(f\"### {evaluator} - Rating: {rating}/10\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(f\"*Evaluation time: {eval_time}s*\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(\"**Explanation:**\")\n",
    "        markdown_lines.append(explanation)\n",
    "        markdown_lines.append(\"\")\n",
    "        \n",
    "        # Optionally include full evaluation if different from explanation\n",
    "        if full_eval != explanation and pd.notna(full_eval):\n",
    "            markdown_lines.append(\"<details>\")\n",
    "            markdown_lines.append(\"<summary>Full Evaluation Text</summary>\")\n",
    "            markdown_lines.append(\"\")\n",
    "            markdown_lines.append(full_eval)\n",
    "            markdown_lines.append(\"\")\n",
    "            markdown_lines.append(\"</details>\")\n",
    "            markdown_lines.append(\"\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    markdown_lines.append(\"---\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(\"## 📊 Summary Statistics\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(f\"- **Number of Evaluators:** {len(df)}\")\n",
    "    markdown_lines.append(f\"- **Average Rating:** {df['rating'].mean():.2f}/10\")\n",
    "    markdown_lines.append(f\"- **Rating Range:** {df['rating'].min()}-{df['rating'].max()}\")\n",
    "    markdown_lines.append(f\"- **Total Evaluation Time:** {df['evaluation_time_s'].sum():.2f}s\")\n",
    "    \n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896c0b8",
   "metadata": {},
   "source": [
    "### Grab a sample question and answer from a couple of the top models to see question, answer, and evaluation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbac704",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_question_markdown = generate_answer_evaluation_markdown(\"Grok 4 Fast\", \"Gemini 2.5 Flash\", ratings_df_full)\n",
    "\n",
    "display(Markdown(hard_question_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the highest rated answers\n",
    "highest_rated_answers = get_highest_rated_answers(ratings_df_full, cost_summary_df)\n",
    "\n",
    "print(\"🏆 Highest Rated Answer for Each Question\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nBest answers selected based on:\")\n",
    "print(\"  1. Highest average rating across all evaluators\")\n",
    "print(\"  2. Lowest cost (in case of rating ties)\")\n",
    "print()\n",
    "\n",
    "display(highest_rated_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_rated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_highest_rated_answers_report(ratings_df: pd.DataFrame, cost_summary_df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive markdown report for all highest-rated answers.\n",
    "    \n",
    "    Loops through the highest_rated_answers DataFrame, generates an evaluation\n",
    "    summary for each question/answer pair, and aggregates them into a single report.\n",
    "    \n",
    "    Args:\n",
    "        ratings_df: DataFrame with all ratings data\n",
    "        cost_summary_df: DataFrame with cost data\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated markdown string containing all answer evaluations\n",
    "    \"\"\"\n",
    "    highest_rated_answers = get_highest_rated_answers(ratings_df, cost_summary_df)\n",
    "\n",
    "    if highest_rated_answers.empty:\n",
    "        return \"⚠️ No highest-rated answers available\"\n",
    "    \n",
    "    markdown_sections = []\n",
    "    \n",
    "    # Add report header\n",
    "    markdown_sections.append(\"## 🏆 Highest Rated Answers - Detailed Evaluations\")\n",
    "    markdown_sections.append(\"\")\n",
    "    markdown_sections.append(f\"This report contains detailed evaluations for the {len(highest_rated_answers)} highest-rated answers.\")\n",
    "    markdown_sections.append(\"\")\n",
    "    markdown_sections.append(\"---\")\n",
    "    markdown_sections.append(\"\")\n",
    "    \n",
    "    # Loop through each highest-rated answer\n",
    "    for idx, row in highest_rated_answers.iterrows():\n",
    "        question_model = row['question_model_name']\n",
    "        answer_model = row['answer_model_name']\n",
    "        avg_rating = row['rating']\n",
    "        cost = row['actual_cost_usd']\n",
    "        \n",
    "        print(f\"Generating evaluation report {idx + 1}/{len(highest_rated_answers)}: {question_model} → {answer_model}\")\n",
    "        \n",
    "        # Add section separator\n",
    "        markdown_sections.append(f\"## 📋 Evaluation {idx + 1} of {len(highest_rated_answers)}\")\n",
    "        markdown_sections.append(f\"**Question by:** {question_model} | **Answer by:** {answer_model} | **Avg Rating:** {avg_rating:.2f}/10 | **Cost:** ${cost:.4f}\")\n",
    "        markdown_sections.append(\"\")\n",
    "        \n",
    "        # Generate the detailed evaluation markdown\n",
    "        evaluation_md = generate_answer_evaluation_markdown(question_model, answer_model, ratings_df)\n",
    "        markdown_sections.append(evaluation_md)\n",
    "        \n",
    "        # Add separator between evaluations\n",
    "        markdown_sections.append(\"\")\n",
    "        markdown_sections.append(\"---\")\n",
    "        markdown_sections.append(\"\")\n",
    "    \n",
    "    # Join all sections into final report\n",
    "    final_report = \"\\n\".join(markdown_sections)\n",
    "    \n",
    "    print(f\"\\n✓ Generated complete report with {len(highest_rated_answers)} detailed evaluations\")\n",
    "    \n",
    "    return final_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf032b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Generate the full markdown report\n",
    "markdown_report = []\n",
    "markdown_report.append(\"# 📋 Comprehensive Model Evaluation Report\\n\")\n",
    "markdown_report.append(top_models_markdown(top_models) + \"\\n\")\n",
    "markdown_report.append(model_performance_summary_markdown(summary_model))\n",
    "markdown_report.append(generate_cost_summary_markdown(cost_summary_df))\n",
    "markdown_report.append(cross_model_rating_matrix_markdown(summary_pair))\n",
    "markdown_report.append(generate_performance_tables_markdown(cost_summary_df))\n",
    "markdown_report.append(cost_breakdown_by_phase_markdown(cost_summary_df))\n",
    "markdown_report.append(most_expensive_calls_markdown(cost_summary_df))\n",
    "markdown_report.append(generate_rating_statistics_markdown(ratings_df))\n",
    "# markdown_report.append(questions_to_markdown(questions_df))\n",
    "markdown_report.append(generate_highest_rated_answers_report(ratings_df_full, cost_summary_df))\n",
    "\n",
    "\n",
    "final_markdown_report = \"\\n\".join(markdown_report)\n",
    "\n",
    "# Display the report in the notebook\n",
    "display(Markdown(final_markdown_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671db28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def save_markdown_report(report: str, folder: str = \"results\", prefix: str = \"evaluation_report\") -> Path:\n",
    "    \"\"\"\n",
    "    Save a markdown string to the results folder with a timestamped filename.\n",
    "    Returns the Path to the saved file.\n",
    "    \"\"\"\n",
    "    # Ensure folder exists\n",
    "    out_dir = Path(folder)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Timestamp safe for filenames\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{prefix}_{ts}.md\"\n",
    "    out_path = out_dir / filename\n",
    "\n",
    "    # Write file\n",
    "    out_path.write_text(report, encoding=\"utf-8\")\n",
    "    print(f\"✓ Saved markdown report to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "report_content = final_markdown_report  # from previous cell 41\n",
    "\n",
    "# Save report\n",
    "saved_path = save_markdown_report(report_content, folder=\"../results\", prefix=\"evaluation_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b59091",
   "metadata": {},
   "source": [
    "## Interpretation & Next Steps\n",
    "\n",
    "### What Do These Results Tell Us?\n",
    "\n",
    "1. **Average Ratings** - Higher average ratings suggest models that consistently provide well-reasoned, complete answers\n",
    "2. **Cross-Model Matrix** - Shows agreement/disagreement between raters. High variance may indicate:\n",
    "   - Different evaluation standards between models\n",
    "   - Genuine quality differences in answers\n",
    "   - Bias toward certain answer styles\n",
    "\n",
    "3. **Self-Ratings vs Peer-Ratings** - Reveals whether models are overconfident, overly critical, or fair\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- **Sample Size**: Results are based on a small number of questions (N models = N questions)\n",
    "- **Question Quality**: The evaluation quality depends on how good the generated questions are\n",
    "- **Rater Bias**: Models may have inherent biases in how they evaluate responses\n",
    "- **Domain Coverage**: Questions may cluster in certain domains based on model training\n",
    "\n",
    "### Ideas for Extension\n",
    "\n",
    "1. **Increase Sample Size**: Run with more questions per model (requires API credits)\n",
    "2. **Domain-Specific Evaluation**: Test models on specific domains (math, coding, creative writing)\n",
    "3. **Human Validation**: Compare model ratings with human expert ratings\n",
    "4. **Consistency Testing**: Run the same evaluation multiple times to check stability\n",
    "5. **Cost Analysis**: Track token usage and costs to compute value-per-dollar\n",
    "6. **Response Time Analysis**: Compare speed vs quality tradeoffs\n",
    "7. **Temperature Experiments**: Test how different temperature settings affect question/answer quality\n",
    "\n",
    "### Saving Results\n",
    "\n",
    "You can export the results for further analysis:\n",
    "\n",
    "```python\n",
    "# Export to CSV\n",
    "summary_model_full.to_csv('model_ratings.csv', index=False)\n",
    "summary_pair_full.to_csv('cross_model_ratings.csv', index=False)\n",
    "questions_df_full.to_csv('generated_questions.csv', index=False)\n",
    "answers_df_full.to_csv('model_answers.csv', index=False)\n",
    "ratings_df_full.to_csv('all_ratings.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7ad14",
   "metadata": {},
   "source": [
    "### Token Usage & Cost Tracking\n",
    "\n",
    "The notebook now captures detailed token usage and cost information for every API call:\n",
    "\n",
    "**What's Tracked:**\n",
    "- **Input tokens**: Tokens sent to the model (prompt + context)\n",
    "- **Output tokens**: Tokens generated by the model (response)\n",
    "- **Total tokens**: Input + output tokens\n",
    "- **Estimated cost**: Based on approximate OpenRouter pricing\n",
    "\n",
    "**Cost Calculation:**\n",
    "- Input tokens: ~$3 per 1M tokens (rough average)\n",
    "- Output tokens: ~$15 per 1M tokens (rough average)\n",
    "- Actual rates vary significantly by model and provider\n",
    "\n",
    "**Export Enhanced Data:**\n",
    "```python\n",
    "# Export all data with token/cost tracking\n",
    "questions_df_full.to_csv('questions_with_costs.csv', index=False)\n",
    "answers_df_full.to_csv('answers_with_costs.csv', index=False) \n",
    "ratings_df_full.to_csv('ratings_with_costs.csv', index=False)\n",
    "cost_summary_df.to_csv('cost_summary.csv', index=False)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Track experiment costs in real-time\n",
    "- Compare cost efficiency across models\n",
    "- Budget for larger evaluations\n",
    "- Identify expensive operations (long answers vs short ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac90abd",
   "metadata": {},
   "source": [
    "## Run Full Evaluation Pipeline\n",
    "\n",
    "Execute the complete pipeline: question generation → answering → rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576696f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete evaluation pipeline with cost tracking\n",
    "# NOTE: Make sure to execute all function definition cells above before running this cell\n",
    "\n",
    "print(f\"Starting evaluation pipeline with {len(top_models)} models...\\n\")\n",
    "\n",
    "# Step 1: Generate questions\n",
    "questions_df_full, valid_questions_df_full = generate_questions(top_models, client)\n",
    "\n",
    "# Step 2: Generate answers\n",
    "answers_df_full = answer_questions(valid_questions_df_full, top_models, client)\n",
    "\n",
    "# Step 3: Evaluate answers\n",
    "ratings_df_full = evaluate_answers(answers_df_full, top_models, client)\n",
    "\n",
    "# Step 4: Generate comprehensive cost and token summary\n",
    "cost_summary_df = generate_cost_summary(questions_df_full, answers_df_full, ratings_df_full)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated artifacts:\")\n",
    "print(\"  - questions_df_full: All generated questions (with token/cost tracking)\")\n",
    "print(\"  - valid_questions_df_full: Successfully generated questions only\")\n",
    "print(\"  - answers_df_full: All model answers to all questions (with token/cost tracking)\")\n",
    "print(\"  - ratings_df_full: All ratings from all rater models (with token/cost tracking)\")\n",
    "print(\"  - summary_model_full: Average rating per model\")\n",
    "print(\"  - summary_pair_full: Cross-model rating matrix\")\n",
    "print(\"  - cost_summary_df: Comprehensive token usage and cost breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c87a4",
   "metadata": {},
   "source": [
    "### 🧪 Quick Test (Optional)\n",
    "\n",
    "Before running the full evaluation, you can test with a single model to verify everything works:\n",
    "\n",
    "```python\n",
    "# Test with just one model\n",
    "test_model = top_models.head(1)\n",
    "print(f\"Testing with: {test_model.iloc[0]['model_name']}\")\n",
    "\n",
    "# Generate one question\n",
    "test_q, test_valid_q = generate_questions(test_model, client)\n",
    "if len(test_valid_q) > 0:\n",
    "    print(\"✓ Question generation works!\")\n",
    "    \n",
    "# Generate one answer\n",
    "test_a = answer_questions(test_valid_q, test_model, client)\n",
    "if len(test_a) > 0:\n",
    "    print(\"✓ Answer generation works!\")\n",
    "        \n",
    "# Generate one rating\n",
    "test_r, _, _ = evaluate_answers(test_a, test_model, client)\n",
    "if len(test_r) > 0:\n",
    "    print(\"✓ Rating works!\")\n",
    "    print(\"\\n🎉 All systems go! Ready for full evaluation.\")\n",
    "```\n",
    "\n",
    "**Tip:** Run this test first if you're unsure about your API setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaabde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings_df_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674daae",
   "metadata": {},
   "source": [
    "\n",
    "# Charts and Graphs\n",
    "Here are some charts that analyze the model performance based on the ratings_df_full dataframe.\n",
    "## Average Rating per Model\n",
    "This chart shows the average rating for each model, providing a clear comparison of their overall performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calculate average rating per model\n",
    "avg_rating_per_model = ratings_df_full.groupby('answer_model_name')['rating'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# Calculate average cost per model\n",
    "avg_cost_per_model = ratings_df_full.groupby('answer_model_name')['actual_cost_usd'].mean().reset_index()\n",
    "\n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(avg_rating_per_model, avg_cost_per_model, on='answer_model_name')\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Bar(x=merged_df['answer_model_name'], y=merged_df['rating'], name='Average Rating'),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=merged_df['answer_model_name'], y=merged_df['actual_cost_usd'], name='Average Cost (USD)', mode='lines+markers'),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text='Average Rating and Cost per Model'\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text='Model')\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text='Average Rating', secondary_y=False)\n",
    "fig.update_yaxes(title_text='Average Cost (USD)', secondary_y=True)\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647f737",
   "metadata": {},
   "source": [
    "\n",
    "## Rating Distribution per Model\n",
    "This box plot shows the distribution of ratings for each model. It helps to visualize the consistency and spread of ratings for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf369b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.box(ratings_df_full,\n",
    "             x='answer_model_name',\n",
    "             y='rating',\n",
    "             title='Rating Distribution per Model',\n",
    "             labels={'rating': 'Rating', 'answer_model_name': 'Model'},\n",
    "             color='answer_model_name')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7e9e0",
   "metadata": {},
   "source": [
    "\n",
    "## Cost vs. Rating\n",
    "This scatter plot shows the relationship between the actual cost in USD and the rating for each model. This can help identify models that are both high-performing and cost-effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3eea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(ratings_df_full,\n",
    "                 x='actual_cost_usd',\n",
    "                 y='rating',\n",
    "                 color='answer_model_name',\n",
    "                 title='Cost vs. Rating',\n",
    "                 labels={'actual_cost_usd': 'Actual Cost (USD)', 'rating': 'Rating'},\n",
    "                 hover_data=['answer_model_name'])\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
