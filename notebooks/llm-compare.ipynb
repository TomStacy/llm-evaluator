{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73993a9",
   "metadata": {},
   "source": [
    "# LLM Model Comparison using OpenRouter Rankings\n",
    "\n",
    "This notebook evaluates and compares the top language models from [OpenRouter Rankings](https://openrouter.ai/rankings).\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Fetches Top Models** - Scrapes the current rankings from OpenRouter to identify the most popular models\n",
    "2. **Generates Questions** - Each model creates a challenging reasoning question\n",
    "3. **Answers Questions** - Each model answers all questions from other models\n",
    "4. **Evaluates Responses** - Each model rates the quality of all answers on a 10-point scale\n",
    "5. **Aggregates Results** - Produces comparison metrics and cross-model rating matrices\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **OpenRouter API Key**: Set the `OPENROUTER_API_KEY` environment variable\n",
    "- **Dependencies**: pandas, requests, beautifulsoup4, playwright, openai (automatically installed with `uv sync`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdd7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports for the notebook\n",
    "import os  # Environment variable access\n",
    "from typing import Any  # Type hints\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd  # DataFrames for structured data\n",
    "\n",
    "# Web scraping and API calls\n",
    "import requests  # HTTP requests for OpenRouter API\n",
    "from bs4 import BeautifulSoup  # HTML parsing for rankings page\n",
    "\n",
    "# Note: Additional imports (re, time, playwright, openai) are imported\n",
    "# within specific cells where they're needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb12d0e",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline Flow\n",
    "\n",
    "Here's how the evaluation works step-by-step:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 1: Fetch Top Models from OpenRouter Rankings     │\n",
    "│  → Scrapes live usage data OR uses API sorting         │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 2: Question Generation (N models)                │\n",
    "│  → Each model creates 1 challenging question           │\n",
    "│  → Total: N questions                                  │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 3: Answer Generation (N × N combinations)        │\n",
    "│  → Each model answers every question                   │\n",
    "│  → Total: N × N answers                                │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 4: Answer Evaluation (N × N ratings)             │\n",
    "│  → Each model rates every answer (1-10 scale)          │\n",
    "│  → Total: N × N ratings                                │\n",
    "└─────────────────┬───────────────────────────────────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  Step 5: Aggregate Results                             │\n",
    "│  → Average ratings per model                           │\n",
    "│  → Cross-model rating matrix                           │\n",
    "│  → Performance insights                                │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Example with 5 models:**\n",
    "- 5 questions generated\n",
    "- 25 answers generated (5 models × 5 questions)\n",
    "- 25 ratings collected (5 models × 5 answers)\n",
    "- Total API calls: ~55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b9032",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Before running this notebook, you need to:\n",
    "\n",
    "1. **Install Playwright browsers** (one-time setup):\n",
    "```bash\n",
    "uv run playwright install chromium\n",
    "```\n",
    "\n",
    "2. **Set your OpenRouter API Key**:\n",
    "   - Get a free API key from [OpenRouter](https://openrouter.ai/)\n",
    "   - Set it as an environment variable:\n",
    "     - **Windows (PowerShell)**: `$env:OPENROUTER_API_KEY=\"your-key-here\"`\n",
    "     - **Mac/Linux**: `export OPENROUTER_API_KEY=\"your-key-here\"`\n",
    "   - Or add it to a `.env` file in the project root\n",
    "\n",
    "The notebook will check for this API key before making requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9705252",
   "metadata": {},
   "source": [
    "## Fetch Top Models from OpenRouter\n",
    "\n",
    "This section scrapes the OpenRouter rankings page to get real-time popularity data based on actual usage.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The `fetch_openrouter_rankings()` function uses **Playwright** (a browser automation tool) to:\n",
    "1. Launch a headless browser\n",
    "2. Navigate to the OpenRouter rankings page\n",
    "3. Wait for JavaScript content to load\n",
    "4. Extract model data including rank, name, token usage, and usage trends\n",
    "\n",
    "**Why Playwright?** The rankings page uses JavaScript to render content dynamically, so we need a real browser to see the data.\n",
    "\n",
    "**Windows Event Loop Note:** Jupyter uses an asyncio event loop that conflicts with Playwright on Windows. The function runs Playwright in a separate thread with its own event loop to avoid this issue.\n",
    "\n",
    "### Available Sorting Options\n",
    "\n",
    "The `fetch_top_openrouter_models()` function supports different sorting criteria:\n",
    "- **popularity** - Models ranked by actual usage on OpenRouter (default, uses Playwright scraping)\n",
    "- **price_low** - Cheapest models first\n",
    "- **price_high** - Most expensive (often most capable) models first  \n",
    "- **context_length** - Longest context window first\n",
    "- **newest** - Most recently added models first\n",
    "\n",
    "**Note:** Only the popularity ranking requires Playwright. Other sorting options use the OpenRouter API directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff56820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback model list used when API calls or web scraping fails\n",
    "FALLBACK_MODELS = [\n",
    "    {\n",
    "        \"id\": \"anthropic/claude-3.5-sonnet\",\n",
    "        \"name\": \"Claude 3.5 Sonnet\",\n",
    "        \"description\": \"Anthropic Claude 3.5 Sonnet\",\n",
    "        \"context_length\": 200000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"openai/gpt-4o\",\n",
    "        \"name\": \"GPT-4o\",\n",
    "        \"description\": \"OpenAI GPT-4o\",\n",
    "        \"context_length\": 128000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"google/gemini-pro-1.5\",\n",
    "        \"name\": \"Gemini Pro 1.5\",\n",
    "        \"description\": \"Google Gemini Pro 1.5\",\n",
    "        \"context_length\": 1_000_000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"meta-llama/llama-3.1-405b-instruct\",\n",
    "        \"name\": \"Llama 3.1 405B\",\n",
    "        \"description\": \"Meta Llama 3.1 405B Instruct\",\n",
    "        \"context_length\": 128000,\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"anthropic/claude-3.5-haiku\",\n",
    "        \"name\": \"Claude 3.5 Haiku\",\n",
    "        \"description\": \"Anthropic Claude 3.5 Haiku\",\n",
    "        \"context_length\": 100000,\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c9b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_openrouter_rankings() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape the OpenRouter rankings page to get the current top models by actual usage.\n",
    "    Uses Playwright async API run in a separate thread to avoid event loop conflicts.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: rank, model_id, model_name, tokens, token_change\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    import concurrent.futures\n",
    "    import sys\n",
    "\n",
    "    async def _async_fetch():\n",
    "        \"\"\"Internal async function to fetch rankings using browser automation\"\"\"\n",
    "        try:\n",
    "            import re\n",
    "\n",
    "            from playwright.async_api import async_playwright\n",
    "\n",
    "            print(\"Launching browser to fetch rankings...\")\n",
    "\n",
    "            async with async_playwright() as p:\n",
    "                # Launch browser in headless mode (no visible window)\n",
    "                browser = await p.chromium.launch(headless=True)\n",
    "                page = await browser.new_page()\n",
    "\n",
    "                try:\n",
    "                    # Navigate to rankings page with short timeout (user can retry if it fails)\n",
    "                    await page.goto(\n",
    "                        \"https://openrouter.ai/rankings\",\n",
    "                        wait_until=\"domcontentloaded\",\n",
    "                        timeout=15000,\n",
    "                    )\n",
    "\n",
    "                    # Wait for the leaderboard section to appear in the DOM\n",
    "                    await page.wait_for_selector(\n",
    "                        \"#leaderboard\", timeout=10000, state=\"attached\"\n",
    "                    )\n",
    "\n",
    "                    # Give JavaScript time to populate the content (5 seconds)\n",
    "                    await page.wait_for_timeout(5000)\n",
    "\n",
    "                    # Get the fully rendered HTML after JavaScript execution\n",
    "                    html = await page.content()\n",
    "\n",
    "                finally:\n",
    "                    await browser.close()\n",
    "\n",
    "            # Parse the rendered HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            leaderboard = soup.find(id=\"leaderboard\")\n",
    "\n",
    "            if not leaderboard:\n",
    "                print(\"⚠ Leaderboard section not found in rendered page\")\n",
    "                return pd.DataFrame(\n",
    "                    columns=[\"rank\", \"model_id\", \"model_name\", \"tokens\", \"token_change\"]\n",
    "                )\n",
    "\n",
    "            # Extract model information using CSS selectors\n",
    "            rankings_data: list[dict[str, object]] = []\n",
    "\n",
    "            # Find all leaderboard entries (each entry is a grid container with 12 columns)\n",
    "            entries = leaderboard.select(\"div.grid.grid-cols-12.items-center\")\n",
    "\n",
    "            for entry in entries[:30]:  # Limit to top 30 models\n",
    "                try:\n",
    "                    # Column 1: Extract rank number (e.g., \"1.\", \"2.\")\n",
    "                    rank_elem = entry.select_one(\"div.col-span-1\")\n",
    "                    rank = (\n",
    "                        int(rank_elem.get_text(strip=True).replace(\".\", \"\"))\n",
    "                        if rank_elem\n",
    "                        else None\n",
    "                    )\n",
    "\n",
    "                    # Column 2: Extract model name and ID from the link\n",
    "                    model_link = entry.select_one(\"div.col-span-7 a.font-medium\")\n",
    "                    if not model_link:\n",
    "                        continue\n",
    "\n",
    "                    model_name = model_link.get_text(strip=True)\n",
    "                    href = model_link.get(\"href\", \"\")\n",
    "                    # Remove leading \"/\" from href to get model_id\n",
    "                    model_id = (\n",
    "                        href[1:] if isinstance(href, str) and href.startswith(\"/\") else href\n",
    "                    )\n",
    "\n",
    "                    # Column 3: Extract token count and change percentage\n",
    "                    tokens: int | None = None\n",
    "                    token_change: str | None = None\n",
    "                    token_container = entry.select_one(\"div.col-span-4\")\n",
    "\n",
    "                    if token_container:\n",
    "                        divs = token_container.select(\"div\")\n",
    "\n",
    "                        # First div contains the token count (e.g., \"1.04T tokens\", \"801B tokens\")\n",
    "                        if divs:\n",
    "                            token_text = divs[0].get_text(strip=True)\n",
    "                            # Parse token count with units (K=thousand, M=million, B=billion, T=trillion)\n",
    "                            token_match = re.search(\n",
    "                                r\"([\\d.]+)([KMBT])\\s*tokens\", token_text, re.IGNORECASE\n",
    "                            )\n",
    "                            if token_match:\n",
    "                                value = float(token_match.group(1))\n",
    "                                unit = token_match.group(2).upper()\n",
    "                                multipliers = {\n",
    "                                    \"K\": 1_000,\n",
    "                                    \"M\": 1_000_000,\n",
    "                                    \"B\": 1_000_000_000,\n",
    "                                    \"T\": 1_000_000_000_000,\n",
    "                                }\n",
    "                                tokens = int(value * multipliers.get(unit, 1))\n",
    "\n",
    "                        # Extract percentage change (usage trend)\n",
    "                        percent_div = token_container.select_one(\"div.mt-1\")\n",
    "                        if percent_div:\n",
    "                            svg_elem = percent_div.select_one(\"svg\")\n",
    "                            full_text = percent_div.get_text(strip=True)\n",
    "                            percent_match = re.search(r\"([\\d.]+)%\", full_text)\n",
    "                            if percent_match:\n",
    "                                percentage_value = percent_match.group(1)\n",
    "                                svg_class = \"\"\n",
    "                                if svg_elem:\n",
    "                                    svg_class_raw = svg_elem.get(\"class\", [])\n",
    "                                    if isinstance(svg_class_raw, list):\n",
    "                                        svg_class = \" \".join(svg_class_raw)\n",
    "                                    else:  # str\n",
    "                                        svg_class = str(svg_class_raw)\n",
    "                                # Red SVG indicates decrease, green indicates increase\n",
    "                                if svg_class and \"text-red\" in svg_class:\n",
    "                                    token_change = f\"-{percentage_value}%\"\n",
    "                                else:\n",
    "                                    token_change = f\"{percentage_value}%\"\n",
    "\n",
    "                    rankings_data.append(\n",
    "                        {\n",
    "                            \"rank\": rank,\n",
    "                            \"model_id\": model_id,\n",
    "                            \"model_name\": model_name,\n",
    "                            \"tokens\": tokens,\n",
    "                            \"token_change\": token_change,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                except (ValueError, AttributeError):\n",
    "                    # Skip entries that don't match expected format\n",
    "                    continue\n",
    "\n",
    "            if rankings_data:\n",
    "                print(\n",
    "                    f\"✓ Successfully extracted {len(rankings_data)} models from rankings page\"\n",
    "                )\n",
    "                return pd.DataFrame(rankings_data)\n",
    "            \n",
    "            print(\"⚠ No models found, using fallback\")\n",
    "            raise ValueError(\"No models extracted\")\n",
    "\n",
    "        except Exception as err:  # noqa: BLE001\n",
    "            print(f\"Error with Playwright: {err}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    def _run_in_thread():\n",
    "        \"\"\"\n",
    "        Run async function in a new event loop in a separate thread.\n",
    "        This avoids conflicts with Jupyter's event loop on Windows.\n",
    "        \"\"\"\n",
    "        # On Windows, use ProactorEventLoop which supports subprocesses\n",
    "        if sys.platform == \"win32\":\n",
    "            loop = asyncio.WindowsProactorEventLoopPolicy().new_event_loop()\n",
    "        else:\n",
    "            loop = asyncio.new_event_loop()\n",
    "\n",
    "        asyncio.set_event_loop(loop)\n",
    "        try:\n",
    "            return loop.run_until_complete(_async_fetch())\n",
    "        finally:\n",
    "            loop.close()\n",
    "\n",
    "    try:\n",
    "        # Run in a thread pool to avoid event loop conflicts with Jupyter\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            future = executor.submit(_run_in_thread)\n",
    "            return future.result(timeout=30)\n",
    "\n",
    "    except Exception as err:  # noqa: BLE001\n",
    "        print(f\"Error fetching rankings: {err}\")\n",
    "\n",
    "        # Fallback to hardcoded list if scraping fails\n",
    "        print(\"Using fallback: hardcoded top models list\")\n",
    "        fallback_rankings = [\n",
    "            {\n",
    "                \"rank\": i + 1,\n",
    "                \"model_id\": model[\"id\"],\n",
    "                \"model_name\": model[\"name\"],\n",
    "                \"tokens\": None,\n",
    "                \"token_change\": None,\n",
    "            }\n",
    "            for i, model in enumerate(FALLBACK_MODELS)\n",
    "        ]\n",
    "        return pd.DataFrame(fallback_rankings)\n",
    "\n",
    "\n",
    "def fetch_top_openrouter_models(top_n: int = 5, sort_by: str = \"popularity\") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Fetch the top N models from OpenRouter using various sorting criteria.\n",
    "\n",
    "    Args:\n",
    "        top_n: Number of top models to return\n",
    "        sort_by: Sorting criterion - 'popularity', 'price_low', 'price_high',\n",
    "                'context_length', 'newest'\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing model information with keys:\n",
    "        id, name, description, context_length, pricing, created, avg_cost\n",
    "    \"\"\"\n",
    "    url = \"https://openrouter.ai/api/v1/models\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract model data from API response\n",
    "        models = data.get(\"data\", [])\n",
    "\n",
    "        # Filter out models without proper pricing or context info\n",
    "        valid_models: list[dict[str, object]] = []\n",
    "        for model in models:\n",
    "            pricing = model.get(\"pricing\", {})\n",
    "            # Only include models with valid pricing information\n",
    "            if pricing and pricing.get(\"prompt\") and pricing.get(\"completion\"):\n",
    "                try:\n",
    "                    prompt_cost = float(pricing.get(\"prompt\", \"0\"))\n",
    "                    completion_cost = float(pricing.get(\"completion\", \"0\"))\n",
    "                except (TypeError, ValueError):\n",
    "                    continue\n",
    "                model_info = {\n",
    "                    \"id\": model.get(\"id\", \"\"),\n",
    "                    \"name\": model.get(\"name\", model.get(\"model_id\", \"\")),\n",
    "                    \"description\": model.get(\n",
    "                        \"description\", \"No description available\"\n",
    "                    ),\n",
    "                    \"context_length\": model.get(\"context_length\", 0),\n",
    "                    \"pricing\": pricing,\n",
    "                    \"created\": model.get(\"created\", 0),\n",
    "                    # Calculate average cost per 1M tokens for easy comparison\n",
    "                    \"avg_cost\": (prompt_cost + completion_cost) / 2 * 1_000_000,\n",
    "                }\n",
    "                valid_models.append(model_info)\n",
    "\n",
    "        # Sort models based on the specified criterion\n",
    "        if sort_by == \"popularity\":\n",
    "            print(\n",
    "                \"Fetching current model rankings from OpenRouter (using Playwright)...\"\n",
    "            )\n",
    "            rankings_df = fetch_openrouter_rankings()\n",
    "            if rankings_df is not None and not rankings_df.empty:\n",
    "                # Create a mapping of model_id to rank\n",
    "                popularity_order = {\n",
    "                    row[\"model_id\"]: row[\"rank\"] for _, row in rankings_df.iterrows()\n",
    "                }\n",
    "                # Sort by rank (lower rank = more popular), default to 999 for unranked\n",
    "                sorted_models = sorted(\n",
    "                    valid_models, key=lambda x: popularity_order.get(x[\"id\"], 999)\n",
    "                )\n",
    "                print(\n",
    "                    f\"✓ Successfully ranked {len(rankings_df)} models by current usage data\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"⚠ Could not fetch rankings, using default sort\")\n",
    "                sorted_models = valid_models\n",
    "        elif sort_by == \"price_low\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"avg_cost\"])\n",
    "        elif sort_by == \"price_high\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"avg_cost\"], reverse=True)\n",
    "        elif sort_by == \"context_length\":\n",
    "            sorted_models = sorted(\n",
    "                valid_models, key=lambda x: x[\"context_length\"], reverse=True\n",
    "            )\n",
    "        elif sort_by == \"newest\":\n",
    "            sorted_models = sorted(valid_models, key=lambda x: x[\"created\"], reverse=True)\n",
    "        else:\n",
    "            sorted_models = valid_models\n",
    "\n",
    "        return sorted_models[:top_n]\n",
    "\n",
    "    except Exception as err:  # noqa: BLE001\n",
    "        print(f\"Error fetching models: {err}\")\n",
    "        # Fallback to a hardcoded list of popular models\n",
    "        return [\n",
    "            {**model, \"pricing\": {}, \"avg_cost\": 0}\n",
    "            for model in FALLBACK_MODELS\n",
    "        ][:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbeae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Fetch and display the current rankings\n",
    "rankings_df = fetch_openrouter_rankings()\n",
    "\n",
    "print(f\"Successfully fetched {len(rankings_df)} ranked models\\n\")\n",
    "print(\"Top 10 Models by Usage on OpenRouter:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display_df = rankings_df.head(10).copy()\n",
    "\n",
    "def format_tokens(tokens: int | None) -> str:\n",
    "    if tokens is None:\n",
    "        return \"N/A\"\n",
    "    if tokens >= 1_000_000_000_000:\n",
    "        return f\"{tokens / 1_000_000_000_000:.2f}T\"\n",
    "    if tokens >= 1_000_000_000:\n",
    "        return f\"{tokens / 1_000_000_000:.1f}B\"\n",
    "    if tokens >= 1_000_000:\n",
    "        return f\"{tokens / 1_000_000:.1f}M\"\n",
    "    return f\"{tokens:,}\"\n",
    "\n",
    "display_df[\"tokens_formatted\"] = display_df[\"tokens\"].apply(format_tokens)\n",
    "display_df[[\"rank\", \"model_name\", \"model_id\", \"tokens_formatted\", \"token_change\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92763d89",
   "metadata": {},
   "source": [
    "### Understanding the Rankings Output\n",
    "\n",
    "The table above shows:\n",
    "\n",
    "- **rank**: Current position on OpenRouter (lower = more popular)\n",
    "- **model_name**: Human-readable name of the model\n",
    "- **model_id**: Unique identifier used for API calls (format: `provider/model-name`)\n",
    "- **tokens_formatted**: Total tokens processed (T=trillion, B=billion, M=million)\n",
    "- **token_change**: Usage trend as percentage change (↑ green positive, ↓ red negative)\n",
    "\n",
    "**What does this tell us?**\n",
    "Models at the top are being used most heavily by real users on OpenRouter, which often (but not always) correlates with quality, speed, or value. This ranking updates in real-time based on actual API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \":free\" suffix from model IDs for cleaner display\n",
    "# OpenRouter sometimes appends \":free\" to free-tier models\n",
    "display_df[\"model_id\"] = display_df[\"model_id\"].str.replace(r\":free$\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23decdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 5 models from the rankings for our evaluation\n",
    "# Alternative approach (commented out): fetch via API with different sorting\n",
    "# top_models = fetch_top_openrouter_models(5, sort_by=\"popularity\")\n",
    "# df_models = pd.DataFrame(top_models)\n",
    "\n",
    "top_models = display_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c313c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that we have the required data for the evaluation\n",
    "print(\"Data Validation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check we have models\n",
    "if len(display_df) < 3:\n",
    "    print(\"⚠️  Warning: Less than 3 models available\")\n",
    "    print(\"   Results may be less meaningful with fewer models\")\n",
    "else:\n",
    "    print(f\"✓ {len(display_df)} models available for evaluation\")\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = ['model_id', 'model_name']\n",
    "missing_cols = [col for col in required_cols if col not in display_df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"✓ All required columns present: {required_cols}\")\n",
    "\n",
    "# Check for duplicate models\n",
    "duplicates = display_df['model_id'].duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"⚠️  Warning: {duplicates} duplicate model IDs found\")\n",
    "else:\n",
    "    print(\"✓ No duplicate models\")\n",
    "\n",
    "print(\"\\nReady to proceed with evaluation! 🚀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9dcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output top_models in markdown table\n",
    "\n",
    "def display_top_models(models):\n",
    "    table = \"| Model | Score |\\n|-------|-------|\\n\"\n",
    "    for model in models:\n",
    "        table += f\"| {model['name']} | {model['score']} |\\n\"\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7253f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert top_models DataFrame to the format expected by display_top_models\n",
    "models_for_display = []\n",
    "for _, model in top_models.iterrows():\n",
    "    models_for_display.append({\n",
    "        'name': model['model_name'],\n",
    "        'score': f\"{model['tokens_formatted']} tokens ({model['token_change']} change)\"\n",
    "})\n",
    "\n",
    "display_top_models(models_for_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the OpenRouter API client\n",
    "# OpenRouter uses an OpenAI-compatible API, so we use the OpenAI Python client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Get API key from environment variable\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    error_msg = \"\"\"\n",
    "    ❌ OPENROUTER_API_KEY not found!\n",
    "    \n",
    "    To fix this issue:\n",
    "    \n",
    "    1. Get a free API key from: https://openrouter.ai/\n",
    "       (Sign up with GitHub or email)\n",
    "    \n",
    "    2. Set the environment variable:\n",
    "       \n",
    "       Windows PowerShell:\n",
    "         $env:OPENROUTER_API_KEY=\"sk-or-v1-xxxxx\"\n",
    "       \n",
    "       Windows CMD:\n",
    "         set OPENROUTER_API_KEY=sk-or-v1-xxxxx\n",
    "       \n",
    "       Mac/Linux:\n",
    "         export OPENROUTER_API_KEY=\"sk-or-v1-xxxxx\"\n",
    "    \n",
    "    3. Restart this notebook kernel (Kernel → Restart)\n",
    "    \n",
    "    Alternative: Create a .env file in the project root:\n",
    "       OPENROUTER_API_KEY=sk-or-v1-xxxxx\n",
    "    \"\"\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Validate API key format\n",
    "if not OPENROUTER_API_KEY.startswith(\"sk-or-\"):\n",
    "    print(\"⚠️  Warning: API key doesn't start with 'sk-or-'\")\n",
    "    print(\"   This might not be a valid OpenRouter API key\")\n",
    "    print(\"   Expected format: sk-or-v1-xxxxx\")\n",
    "\n",
    "# Create client with OpenRouter's base URL\n",
    "client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OPENROUTER_API_KEY)\n",
    "\n",
    "print(\"✓ OpenRouter client configured successfully!\")\n",
    "print(f\"  API key: {OPENROUTER_API_KEY[:15]}...{OPENROUTER_API_KEY[-4:]}\")\n",
    "print(\"  Base URL: https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0d2fa",
   "metadata": {},
   "source": [
    "## Model Comparison Pipeline\n",
    "\n",
    "The evaluation consists of three phases:\n",
    "\n",
    "1. **Question Generation** - Each model creates one challenging reasoning question\n",
    "2. **Answer Generation** - Each model answers every question from all other models  \n",
    "3. **Answer Evaluation** - Each model rates every answer on a 10-point scale\n",
    "\n",
    "This creates a comprehensive cross-comparison where models evaluate each other's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2056374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "# Display how many models we're using for the evaluation pipeline\n",
    "num_models = len(top_models)\n",
    "total_questions = num_models\n",
    "total_answers = num_models * num_models\n",
    "total_ratings = num_models * num_models\n",
    "total_api_calls = total_questions + total_answers + total_ratings\n",
    "\n",
    "print(f\"Using {num_models} models for the evaluation pipeline:\")\n",
    "print(\"  - Each model will generate 1 question\")\n",
    "print(f\"  - Each model will answer {num_models} questions\")\n",
    "print(f\"  - Each model will rate {num_models} answers\")\n",
    "print(f\"\\nTotal API calls: {total_api_calls}\")\n",
    "print(f\"  • {total_questions} question generation calls\")\n",
    "print(f\"  • {total_answers} answer generation calls\")\n",
    "print(f\"  • {total_ratings} rating calls\")\n",
    "\n",
    "# Estimate time based on typical API response times\n",
    "avg_question_time = 3  # seconds\n",
    "avg_answer_time = 5    # seconds\n",
    "avg_rating_time = 2    # seconds\n",
    "\n",
    "estimated_time = (\n",
    "    total_questions * avg_question_time +\n",
    "    total_answers * avg_answer_time +\n",
    "    total_ratings * avg_rating_time\n",
    ")\n",
    "\n",
    "print(f\"\\n⏱️  Estimated total time: ~{estimated_time // 60} minutes {estimated_time % 60} seconds\")\n",
    "print(f\"   (assuming {avg_question_time}s/question, {avg_answer_time}s/answer, {avg_rating_time}s/rating)\")\n",
    "print(\"\\n💡 Tip: Actual time may vary based on model speed and API load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(\n",
    "    models: pd.DataFrame, client: Any\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate evaluation questions using the provided models.\n",
    "    \n",
    "    Each model generates one challenging question designed to test reasoning depth.\n",
    "    \n",
    "    Args:\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (all_questions_df, valid_questions_df)\n",
    "        - all_questions_df: All questions including any that had errors\n",
    "        - valid_questions_df: Only successfully generated questions (errors filtered out)\n",
    "        \n",
    "    Example:\n",
    "        >>> all_q, valid_q = generate_questions(top_models, client)\n",
    "        >>> print(f\"Generated {len(valid_q)} valid questions out of {len(all_q)} attempts\")\n",
    "        Generated 5 valid questions out of 5 attempts\n",
    "        \n",
    "    Notes:\n",
    "        - Uses temperature=0.8 for creative/diverse questions\n",
    "        - Each question is limited to 180 tokens\n",
    "        - Errors are captured in the DataFrame but don't stop execution\n",
    "        - Preview of each question is printed during generation\n",
    "    \"\"\"\n",
    "    question_generation_prompt = (\n",
    "        \"Please craft ONE challenging, original, nuanced question that can effectively \"\n",
    "        \"discriminate between language models of varying reasoning depth. The question should: \"\n",
    "        \"(1) require multi-step reasoning, (2) avoid simple trivia, (3) be answerable without external \"\n",
    "        \"browsing, (4) not be purely opinion, (5) allow partial credit, and (6) be less than 400 tokens. Provide only the question text.\"\n",
    "    )\n",
    "\n",
    "    generated_questions: list[dict[str, Any]] = []\n",
    "    \n",
    "    # Each model generates one question\n",
    "    for _, model in models.iterrows():\n",
    "        mid = model[\"model_id\"]\n",
    "        mname = model.get(\"model_name\", mid)\n",
    "        print(f\"\\n[Generation] {mname} generating a question...\")\n",
    "        \n",
    "        try:\n",
    "            start = time.time()\n",
    "            completion = client.chat.completions.create(\n",
    "                model=mid,\n",
    "                messages=[{\"role\": \"user\", \"content\": question_generation_prompt}],\n",
    "                max_tokens=10000,  # Limit response length (increased to 10,000 for Gemini 2.5 Pro despite the instructions in the prompt)\n",
    "                temperature=0.8,  # Higher temperature for creative/diverse questions\n",
    "            )\n",
    "            q_text = completion.choices[0].message.content.strip()\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            generated_questions.append(\n",
    "                {\n",
    "                    \"question_model_id\": mid,\n",
    "                    \"question_model_name\": mname,\n",
    "                    \"question\": q_text,\n",
    "                    \"gen_time_s\": round(elapsed, 2),\n",
    "                }\n",
    "            )\n",
    "            # Show preview of generated question\n",
    "            print(\n",
    "                f\"✓ Question from {mname}: {q_text[:110]}{'...' if len(q_text) > 110 else ''}\"\n",
    "            )\n",
    "        except Exception as err:  # noqa: BLE001\n",
    "            # Record errors but continue with other models\n",
    "            generated_questions.append(\n",
    "                {\n",
    "                    \"question_model_id\": mid,\n",
    "                    \"question_model_name\": mname,\n",
    "                    \"question\": f\"Error generating question: {err}\",\n",
    "                    \"gen_time_s\": None,\n",
    "                }\n",
    "            )\n",
    "            print(f\"✗ {mname} failed: {err}\")\n",
    "\n",
    "    questions_df = pd.DataFrame(generated_questions)\n",
    "\n",
    "    # Filter out errored questions for downstream phases\n",
    "    valid_questions_df = questions_df[\n",
    "        ~questions_df[\"question\"].str.startswith(\"Error\")\n",
    "    ].copy()\n",
    "\n",
    "    return questions_df, valid_questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df, valid_questions_df = generate_questions(top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the question from the 4th row (index 4) - Gemini 2.5 Pro in this case because we had to increase max_tokens\n",
    "if len(questions_df) >= 5:\n",
    "    fourth_question = questions_df.iloc[3]\n",
    "    print(f\"Question from {fourth_question['question_model_name']}:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(fourth_question['question'])\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Generation time: {fourth_question['gen_time_s']}s\")\n",
    "else:\n",
    "    print(f\"Only {len(questions_df)} questions available. Cannot display 4th question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31571630",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cac19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(\n",
    "    questions_df: pd.DataFrame, models: pd.DataFrame, client: Any\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate answers to questions using the provided models.\n",
    "\n",
    "    Each model answers every question from every model (including its own question).\n",
    "\n",
    "    Args:\n",
    "        questions_df: DataFrame with questions (must have 'question' and 'question_model_name' columns)\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with answers containing columns: question_model_name, question,\n",
    "        answer_model_id, answer_model_name, answer, answer_time_s\n",
    "    \"\"\"\n",
    "    answers: list[dict[str, Any]] = []\n",
    "    answer_instructions = (\n",
    "        \"You will be given a question designed to evaluate reasoning depth. Provide a thorough, \"\n",
    "        \"structured answer. Show reasoning explicitly if helpful, but keep it concise and logical.\"\n",
    "    )\n",
    "\n",
    "    # Each model answers each question\n",
    "    for _, qrow in questions_df.iterrows():\n",
    "        q_text = qrow[\"question\"]\n",
    "        origin_model = qrow[\"question_model_name\"]\n",
    "\n",
    "        for _, model in models.iterrows():\n",
    "            mid = model[\"model_id\"]\n",
    "            mname = model.get(\"model_name\", mid)\n",
    "            print(f\"\\n[Answer] {mname} answering question from {origin_model}...\")\n",
    "\n",
    "            try:\n",
    "                start = time.time()\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=mid,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": answer_instructions},\n",
    "                        {\"role\": \"user\", \"content\": q_text},\n",
    "                    ],\n",
    "                    max_tokens=10000,  # Allow longer responses for thorough answers (especially for Gemini Pro)\n",
    "                    temperature=0.5,  # Moderate temperature for balanced responses\n",
    "                    timeout=30.0,  # 30 second timeout to prevent hanging\n",
    "                )\n",
    "                ans_text = completion.choices[0].message.content.strip()\n",
    "                elapsed = time.time() - start\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"question_model_name\": origin_model,\n",
    "                        \"question\": q_text,\n",
    "                        \"answer_model_id\": mid,\n",
    "                        \"answer_model_name\": mname,\n",
    "                        \"answer\": ans_text,\n",
    "                        \"answer_time_s\": round(elapsed, 2),\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✓ Answer length: {len(ans_text)} chars\")\n",
    "            except Exception as err:  # noqa: BLE001\n",
    "                # Record errors but continue with other models\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"question_model_name\": origin_model,\n",
    "                        \"question\": q_text,\n",
    "                        \"answer_model_id\": mid,\n",
    "                        \"answer_model_name\": mname,\n",
    "                        \"answer\": f\"Error: {err}\",\n",
    "                        \"answer_time_s\": None,\n",
    "                    }\n",
    "                )\n",
    "                print(f\"✗ {mname} failed to answer: {err}\")\n",
    "\n",
    "    answers_df = pd.DataFrame(answers)\n",
    "    print(\"\\n✓ Answers DataFrame ready (answers_df)\")\n",
    "\n",
    "    return answers_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df_full = answer_questions(valid_questions_df, top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74afedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(\n",
    "    answers_df: pd.DataFrame, models: pd.DataFrame, client: Any\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate answers using the provided models as raters.\n",
    "    \n",
    "    Each model rates every answer on a 10-point scale across multiple criteria.\n",
    "    \n",
    "    Args:\n",
    "        answers_df: DataFrame with answers (must have 'question', 'answer', and 'answer_model_name' columns)\n",
    "        models: DataFrame with model information (must have 'model_id' and 'model_name' columns)\n",
    "        client: OpenRouter API client (OpenAI-compatible)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (ratings_df, summary_by_model, summary_by_pair)\n",
    "        - ratings_df: All individual ratings\n",
    "        - summary_by_model: Average rating per answer model\n",
    "        - summary_by_pair: Cross-model rating matrix (how each rater rated each answerer)\n",
    "    \"\"\"\n",
    "    ratings: list[dict[str, Any]] = []\n",
    "    rating_prompt_template = (\n",
    "        \"You are evaluating an answer to a reasoning-focused question. Score 1-10 (10 = outstanding).\\n\"\n",
    "        \"Criteria (roughly equal weight):\\n\"\n",
    "        \"1. Clarity & organization\\n\"\n",
    "        \"2. Depth & correctness\\n\"\n",
    "        \"3. Completeness\\n\"\n",
    "        \"4. Insight/originality (if applicable)\\n\"\n",
    "        \"\\nReturn ONLY the integer score (1-10).\"\n",
    "    )\n",
    "\n",
    "    # Each model rates each answer\n",
    "    for _, arow in answers_df.iterrows():\n",
    "        ans_text = arow[\"answer\"]\n",
    "        # Skip error responses\n",
    "        if ans_text.startswith(\"Error:\"):\n",
    "            continue\n",
    "            \n",
    "        q_text = arow[\"question\"]\n",
    "        ans_model = arow[\"answer_model_name\"]\n",
    "        \n",
    "        for _, model in models.iterrows():\n",
    "            mid = model[\"model_id\"]\n",
    "            mname = model.get(\"model_name\", mid)\n",
    "            print(f\"\\n[Rating] {mname} rating answer from {ans_model}...\")\n",
    "            \n",
    "            # Combine question, answer, and rating instructions\n",
    "            rating_input = (\n",
    "                f\"Question: {q_text}\\n\\nAnswer: {ans_text}\\n\\n{rating_prompt_template}\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=mid,\n",
    "                    messages=[{\"role\": \"user\", \"content\": rating_input}],\n",
    "                    max_tokens=1000,  # Very short response (just a number)\n",
    "                    temperature=0.0,  # Deterministic rating\n",
    "                )\n",
    "                raw = completion.choices[0].message.content.strip()\n",
    "                \n",
    "                # Extract numeric score (1-10) from response\n",
    "                match = re.search(r\"\\b(10|[1-9])\\b\", raw)\n",
    "                score = int(match.group(1)) if match else None\n",
    "            except Exception as err:  # noqa: BLE001\n",
    "                raw = f\"Error: {err}\"\n",
    "                score = None\n",
    "                \n",
    "            ratings.append(\n",
    "                {\n",
    "                    \"question\": q_text,\n",
    "                    \"answer_model_name\": ans_model,\n",
    "                    \"rater_model_id\": mid,\n",
    "                    \"rater_model_name\": mname,\n",
    "                    \"raw_rating_text\": raw,\n",
    "                    \"rating\": score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    ratings_df = pd.DataFrame(ratings)\n",
    "    print(\"\\n✓ Ratings DataFrame ready (ratings_df)\")\n",
    "\n",
    "    # Generate aggregation summaries\n",
    "    summary_model = pd.DataFrame()\n",
    "    summary_pair = pd.DataFrame()\n",
    "\n",
    "    if not ratings_df.empty:\n",
    "        # Calculate average rating for each model's answers\n",
    "        summary_model = (\n",
    "            ratings_df.groupby(\"answer_model_name\")[\"rating\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"rating\": \"avg_rating\"})\n",
    "        )\n",
    "        \n",
    "        # Calculate average rating for each (answerer, rater) pair\n",
    "        summary_pair = (\n",
    "            ratings_df.groupby([\"answer_model_name\", \"rater_model_name\"])[\"rating\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"rating\": \"avg_rating\"})\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Average rating per answer model:\")\n",
    "        print(\"=\"*80)\n",
    "        display(summary_model.sort_values(\"avg_rating\", ascending=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Cross-model rating matrix (long form):\")\n",
    "        print(\"=\"*80)\n",
    "        display(summary_pair.head(20))\n",
    "    else:\n",
    "        print(\"⚠ No ratings captured.\")\n",
    "\n",
    "    return ratings_df, summary_model, summary_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25f3d9",
   "metadata": {},
   "source": [
    "### Understanding the Rating System\n",
    "\n",
    "Each model evaluates answers on a **10-point scale** across these criteria:\n",
    "\n",
    "1. **Clarity & Organization** (2.5 points)\n",
    "   - Is the answer well-structured and easy to follow?\n",
    "   - Are concepts explained clearly without unnecessary jargon?\n",
    "\n",
    "2. **Depth & Correctness** (2.5 points)\n",
    "   - Is the reasoning sound and logically valid?\n",
    "   - Are facts accurate and relevant?\n",
    "\n",
    "3. **Completeness** (2.5 points)\n",
    "   - Does the answer address all parts of the question?\n",
    "   - Are edge cases or caveats mentioned when appropriate?\n",
    "\n",
    "4. **Insight & Originality** (2.5 points)\n",
    "   - Does the answer provide novel perspectives or connections?\n",
    "   - Is there evidence of deeper understanding beyond surface-level knowledge?\n",
    "\n",
    "**Why use models as raters?**\n",
    "- Consistent evaluation criteria across all answers\n",
    "- Faster than human evaluation for large-scale comparisons\n",
    "- Tests if models can accurately judge reasoning quality (meta-evaluation)\n",
    "\n",
    "**Limitations:**\n",
    "- Models may have biases (e.g., preferring similar styles to their own)\n",
    "- Some subjective criteria may be interpreted differently\n",
    "- This is why we aggregate ratings across multiple model-raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae87169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df_full, summary_model_full, summary_pair_full = evaluate_answers(answers_df_full, top_models, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432188cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pair_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf032b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for README documentation\n",
    "print(\"## Generated Questions\\n\")\n",
    "for idx, row in valid_questions_df.iterrows():\n",
    "    print(f\"### {idx+1}. {row['question_model_name']}\")\n",
    "    print(f\"**Question:** {row['question']}\\n\")\n",
    "    print(f\"**Generation Time:** {row['gen_time_s']}s\\n\")\n",
    "\n",
    "print(\"\\n## Model Performance Summary\\n\")\n",
    "print(\"### Overall Average Ratings (10-point scale)\\n\")\n",
    "for idx, row in summary_model_full.sort_values('avg_rating', ascending=False).iterrows():\n",
    "    print(f\"{idx+1}. **{row['answer_model_name']}**: {row['avg_rating']:.2f}/10\")\n",
    "\n",
    "print(\"\\n### Cross-Model Rating Matrix\")\n",
    "print(\"\\nThis shows how each model (rater/columns) rated each model's answers (answerer/rows):\\n\")\n",
    "\n",
    "# Create a pivot table for easier viewing\n",
    "pivot_table = summary_pair_full.pivot_table(\n",
    "    index='answer_model_name',\n",
    "    columns='rater_model_name',\n",
    "    values='avg_rating'\n",
    ")\n",
    "\n",
    "# Convert pivot table to markdown format\n",
    "def pivot_to_markdown(pivot_df):\n",
    "    \"\"\"Convert a pivot table DataFrame to markdown table format.\"\"\"\n",
    "    if pivot_df.empty:\n",
    "        return \"No data available\"\n",
    "    \n",
    "    # Start with header row\n",
    "    headers = ['Model'] + list(pivot_df.columns)\n",
    "    markdown_lines = []\n",
    "    \n",
    "    # Create header\n",
    "    header_line = '| ' + ' | '.join(headers) + ' |'\n",
    "    markdown_lines.append(header_line)\n",
    "    \n",
    "    # Create separator line\n",
    "    separator = '| ' + ' | '.join(['---'] * len(headers)) + ' |'\n",
    "    markdown_lines.append(separator)\n",
    "    \n",
    "    # Add data rows\n",
    "    for index, row in pivot_df.iterrows():\n",
    "        row_data = [str(index)]\n",
    "        for col in pivot_df.columns:\n",
    "            value = row[col]\n",
    "            if pd.isna(value):\n",
    "                row_data.append('N/A')\n",
    "            else:\n",
    "                row_data.append(f'{value:.2f}')\n",
    "        row_line = '| ' + ' | '.join(row_data) + ' |'\n",
    "        markdown_lines.append(row_line)\n",
    "    \n",
    "    return '\\n'.join(markdown_lines)\n",
    "\n",
    "print(pivot_to_markdown(pivot_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b59091",
   "metadata": {},
   "source": [
    "## Interpretation & Next Steps\n",
    "\n",
    "### What Do These Results Tell Us?\n",
    "\n",
    "1. **Average Ratings** - Higher average ratings suggest models that consistently provide well-reasoned, complete answers\n",
    "2. **Cross-Model Matrix** - Shows agreement/disagreement between raters. High variance may indicate:\n",
    "   - Different evaluation standards between models\n",
    "   - Genuine quality differences in answers\n",
    "   - Bias toward certain answer styles\n",
    "\n",
    "3. **Self-Ratings vs Peer-Ratings** - Reveals whether models are overconfident, overly critical, or fair\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- **Sample Size**: Results are based on a small number of questions (N models = N questions)\n",
    "- **Question Quality**: The evaluation quality depends on how good the generated questions are\n",
    "- **Rater Bias**: Models may have inherent biases in how they evaluate responses\n",
    "- **Domain Coverage**: Questions may cluster in certain domains based on model training\n",
    "\n",
    "### Ideas for Extension\n",
    "\n",
    "1. **Increase Sample Size**: Run with more questions per model (requires API credits)\n",
    "2. **Domain-Specific Evaluation**: Test models on specific domains (math, coding, creative writing)\n",
    "3. **Human Validation**: Compare model ratings with human expert ratings\n",
    "4. **Consistency Testing**: Run the same evaluation multiple times to check stability\n",
    "5. **Cost Analysis**: Track token usage and costs to compute value-per-dollar\n",
    "6. **Response Time Analysis**: Compare speed vs quality tradeoffs\n",
    "7. **Temperature Experiments**: Test how different temperature settings affect question/answer quality\n",
    "\n",
    "### Saving Results\n",
    "\n",
    "You can export the results for further analysis:\n",
    "\n",
    "```python\n",
    "# Export to CSV\n",
    "summary_model_full.to_csv('model_ratings.csv', index=False)\n",
    "summary_pair_full.to_csv('cross_model_ratings.csv', index=False)\n",
    "questions_df_full.to_csv('generated_questions.csv', index=False)\n",
    "answers_df_full.to_csv('model_answers.csv', index=False)\n",
    "ratings_df_full.to_csv('all_ratings.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac90abd",
   "metadata": {},
   "source": [
    "## Run Full Evaluation Pipeline\n",
    "\n",
    "Execute the complete pipeline: question generation → answering → rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576696f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete evaluation pipeline\n",
    "print(f\"Starting evaluation pipeline with {len(top_models)} models...\\n\")\n",
    "\n",
    "# Step 1: Generate questions\n",
    "questions_df_full, valid_questions_df_full = generate_questions(top_models, client)\n",
    "\n",
    "# Step 2: Generate answers\n",
    "answers_df_full = answer_questions(valid_questions_df_full, top_models, client)\n",
    "\n",
    "# Step 3: Evaluate answers\n",
    "ratings_df_full, summary_model_full, summary_pair_full = evaluate_answers(answers_df_full, top_models, client)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated artifacts:\")\n",
    "print(\"  - questions_df_full: All generated questions\")\n",
    "print(\"  - valid_questions_df_full: Successfully generated questions only\")\n",
    "print(\"  - answers_df_full: All model answers to all questions\")\n",
    "print(\"  - ratings_df_full: All ratings from all rater models\")\n",
    "print(\"  - summary_model_full: Average rating per model\")\n",
    "print(\"  - summary_pair_full: Cross-model rating matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display interesting statistics about the ratings in markdown format\n",
    "def generate_rating_statistics_markdown():\n",
    "    \"\"\"Generate rating statistics in markdown format.\"\"\"\n",
    "    if ratings_df_full.empty or 'rating' not in ratings_df_full.columns:\n",
    "        return \"⚠️ **No valid ratings data available**\"\n",
    "    \n",
    "    # Overall statistics\n",
    "    valid_ratings = ratings_df_full['rating'].dropna()\n",
    "    \n",
    "    markdown_lines = []\n",
    "    markdown_lines.append(\"## Rating Statistics Summary\")\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    markdown_lines.append(\"### 📊 Overall Rating Distribution\")\n",
    "    markdown_lines.append(\"\")\n",
    "    markdown_lines.append(f\"- **Total ratings collected**: {len(valid_ratings)}\")\n",
    "    markdown_lines.append(f\"- **Average rating**: {valid_ratings.mean():.2f} / 10\")\n",
    "    markdown_lines.append(f\"- **Median rating**: {valid_ratings.median():.1f} / 10\")\n",
    "    markdown_lines.append(f\"- **Standard deviation**: {valid_ratings.std():.2f}\")\n",
    "    markdown_lines.append(f\"- **Range**: {valid_ratings.min():.0f} - {valid_ratings.max():.0f}\")\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Rating distribution\n",
    "    markdown_lines.append(\"### 📈 Rating Frequency\")\n",
    "    markdown_lines.append(\"\")\n",
    "    rating_counts = valid_ratings.value_counts().sort_index(ascending=False)\n",
    "    for score, count in rating_counts.items():\n",
    "        bar = '█' * int(count / len(valid_ratings) * 20)  # Shorter bars for markdown\n",
    "        percentage = count/len(valid_ratings)*100\n",
    "        markdown_lines.append(f\"- **{score:2.0f}/10**: `{bar}` ({count} ratings, {percentage:.1f}%)\")\n",
    "    markdown_lines.append(\"\")\n",
    "    \n",
    "    # Self-rating analysis (do models rate themselves higher?)\n",
    "    if 'answer_model_name' in ratings_df_full.columns and 'rater_model_name' in ratings_df_full.columns:\n",
    "        self_ratings = ratings_df_full[\n",
    "            ratings_df_full['answer_model_name'] == ratings_df_full['rater_model_name']\n",
    "        ]['rating'].dropna()\n",
    "        \n",
    "        other_ratings = ratings_df_full[\n",
    "            ratings_df_full['answer_model_name'] != ratings_df_full['rater_model_name']\n",
    "        ]['rating'].dropna()\n",
    "        \n",
    "        if len(self_ratings) > 0 and len(other_ratings) > 0:\n",
    "            markdown_lines.append(\"### 🤔 Self-Rating vs. Peer-Rating\")\n",
    "            markdown_lines.append(\"\")\n",
    "            markdown_lines.append(f\"- **Average when rating own answers**: {self_ratings.mean():.2f}\")\n",
    "            markdown_lines.append(f\"- **Average when rating others' answers**: {other_ratings.mean():.2f}\")\n",
    "            diff = self_ratings.mean() - other_ratings.mean()\n",
    "            \n",
    "            if abs(diff) < 0.5:\n",
    "                bias_text = f\"Models are fairly unbiased (difference: {diff:+.2f})\"\n",
    "            elif diff > 0:\n",
    "                bias_text = f\"Models tend to rate themselves higher (difference: {diff:+.2f})\"\n",
    "            else:\n",
    "                bias_text = f\"Models tend to rate themselves lower (difference: {diff:+.2f})\"\n",
    "            \n",
    "            markdown_lines.append(f\"- **Bias Analysis**: {bias_text}\")\n",
    "            markdown_lines.append(\"\")\n",
    "    \n",
    "    # Most generous and strictest raters\n",
    "    if 'rater_model_name' in ratings_df_full.columns:\n",
    "        avg_by_rater = ratings_df_full.groupby('rater_model_name')['rating'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        markdown_lines.append(\"### 🎭 Rater Characteristics\")\n",
    "        markdown_lines.append(\"\")\n",
    "        markdown_lines.append(f\"- **🎁 Most Generous Rater**: {avg_by_rater.index[0]} (avg: {avg_by_rater.iloc[0]:.2f})\")\n",
    "        markdown_lines.append(f\"- **🔍 Strictest Rater**: {avg_by_rater.index[-1]} (avg: {avg_by_rater.iloc[-1]:.2f})\")\n",
    "        markdown_lines.append(f\"- **📏 Rater Spread**: {avg_by_rater.iloc[0] - avg_by_rater.iloc[-1]:.2f} points\")\n",
    "    \n",
    "    return '\\n'.join(markdown_lines)\n",
    "\n",
    "# Generate and print the markdown statistics\n",
    "print(generate_rating_statistics_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c87a4",
   "metadata": {},
   "source": [
    "### 🧪 Quick Test (Optional)\n",
    "\n",
    "Before running the full evaluation, you can test with a single model to verify everything works:\n",
    "\n",
    "```python\n",
    "# Test with just one model\n",
    "test_model = top_models.head(1)\n",
    "print(f\"Testing with: {test_model.iloc[0]['model_name']}\")\n",
    "\n",
    "# Generate one question\n",
    "test_q, test_valid_q = generate_questions(test_model, client)\n",
    "if len(test_valid_q) > 0:\n",
    "    print(\"✓ Question generation works!\")\n",
    "    \n",
    "# Generate one answer\n",
    "test_a = answer_questions(test_valid_q, test_model, client)\n",
    "if len(test_a) > 0:\n",
    "    print(\"✓ Answer generation works!\")\n",
    "        \n",
    "# Generate one rating\n",
    "test_r, _, _ = evaluate_answers(test_a, test_model, client)\n",
    "if len(test_r) > 0:\n",
    "    print(\"✓ Rating works!\")\n",
    "    print(\"\\n🎉 All systems go! Ready for full evaluation.\")\n",
    "```\n",
    "\n",
    "**Tip:** Run this test first if you're unsure about your API setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
